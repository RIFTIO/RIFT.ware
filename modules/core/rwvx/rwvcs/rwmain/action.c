
/*
 * 
 *   Copyright 2016 RIFT.IO Inc
 *
 *   Licensed under the Apache License, Version 2.0 (the "License");
 *   you may not use this file except in compliance with the License.
 *   You may obtain a copy of the License at
 *
 *       http://www.apache.org/licenses/LICENSE-2.0
 *
 *   Unless required by applicable law or agreed to in writing, software
 *   distributed under the License is distributed on an "AS IS" BASIS,
 *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *   See the License for the specific language governing permissions and
 *   limitations under the License.
 *
 *
 */

#include <stdio.h>
#include <signal.h>
#include <string.h>
#include <sys/prctl.h>
#include <sys/stat.h>
#include <sys/types.h>
#include <unistd.h>
#include <wait.h>
#include <termios.h>
#include <sys/ioctl.h>

#include <reaper_client.h>
#include <rwlib.h>
#include <rwnetns.h>
#include <rw_sys.h>
#include <rw-vm-heartbeat-log.pb-c.h>
#include <rw-log.pb-c.h>

#include <rw-manifest.pb-c.h>
#include <rw-vcs.pb-c.h>
#include <rwsched.h>
#include <rwvcs.h>
#include <rwvcs_component.h>
#include <rwvcs_manifest.h>
#include <rwvcs_rwzk.h>
#include <rwvx.h>

#include "rwmain.h"
#include "serf_client.h"
#include "redis_client.h"
#include "rwha_dts_api.h"

#define UINT_STR_SZ  16
#define RW_ZK_TRANSITION_LOCK_PATH "/RW/TRANSITION_LOCK"
/*
 * Controls if terminal interaction required for the native process and what
 * type of terminal (use-parent or use-pseudo)
 */
typedef enum  {
  NATIVE_PROC_TERM_MODE_NONE,    /* Non interactive */
  NATIVE_PROC_TERM_MODE_INTERACTIVE   /* Interactive and needs pseudo terminal */
} native_proc_term_mode_t;

/* Argument to launch_process() containing paths for the redirection
 * of stdout/stderr
 */
struct stream_paths {
  const char * stdout_path;
  const char * stderr_path;
};

/*
 * A structure for holding the result of iteration
 * over zookeeper component list.
 */
typedef struct restart_list_answer {
  rwmain_restart_instance_t* restart_list;
  int index;
} restart_list_answer;

/*
 * State within notify_transition function.
 */
typedef enum {
  STARTING = 0,
  HANDLE_COMMON_TASKS,
  WAIT_FOR_OTHERS,
} transition_state_t;

/*
 * For maintaining the state information during
 * transitioning phase.
 */
typedef struct transition_state_info {
  struct rwmain_gi  *rwmain;
  vcs_vm_state       vm_state;
  transition_state_t fsm_state;
} transition_state_info;

/*
 * Execute all the entries in the specified event
 *
 * @param instance  - rwvcs instance
 * @param parent_id - instance name of the parent of this component
 * @param m_event   - event to execute
 * @return          - RW_STATUS_SUCCESS
 *                    aborts on failure
 */
rw_status_t rwvcs_event_run(
    rwvcs_instance_ptr_t instance,
    const char * parent_id,
    vcs_manifest_event *m_event);

/*
 * This is the main function starting serf agents. The serf
 * agents will be forked. The parent process will need to connect
 * to the serf rpc for listening events and sending data to serf
 * agents. The serf agent needs the current address of the vm
 * which is the vm_ip_address. The lead_vm_ip_address ip adress
 * is when this serf agent will join to the serf cluster. if this
 * is the lead vm, then lead_vm_ip_address will be vm_ip_address,
 * otherwise, it is passed as NULL initially. The
 * lead_vm_ip_address will obtained from the zookeeper who is the
 * lead VM in the cluster or colony. It is possible that if there
 * are no lead VM, then Serf agent will not be started.  For
 * collapsed model, both vm_ip_address and lead_vm_ip_address are
 * generated
 *
 * @param rwmain               - rwmain instance
 * @param vm_ip_address        - This is the vm ip address
 * @param lead_vm_ip_address   - This is the lead vm ip address.
 * @param instance_name        - This is the instance name
 * @return                     - RW_STATUS_SUCCESS.  On any failure this function
 *                               currently will just assert as they are either parsing
 *                               or allocation errors.
 */
static rw_status_t heartbeatmon_start(
    struct rwmain_gi * rwmain,
    char *vm_ip_address,
    char *lead_vm_ip_address,
    const char * instance_name);


/*
 * Generate an instance id for the given start action.  If the action specified
 * both the instance-id and instance-name, those will be returned.  If those
 * fields depend on python-variables being evaluated then that needs to have
 * happened first.  Otherwise, a new identifier is allocated and the
 * instance-name is generated by using that and the component-name.  The
 * instance_name will be allocated and it is the responsibility of the caller to
 * deallocate it when appropriate.
 *
 *
 * @param rwvcs         - rwvcs instance
 * @param start         - start action
 * @param instance_name - on success, a newly allocated string containing the
 *                        instance-name.  This string is owned by the caller.
 * @param instance_id   - on success, the instance id for the started component.
 * @return              - RW_STATUS_SUCCESS.  On any failure this function
 *                        currently will just assert as they are either parsing
 *                        or allocation errors.
 */
static rw_status_t generate_instance_name_id(
    rwvcs_instance_ptr_t rwvcs,
    vcs_manifest_action_start * start,
    char ** instance_name,
    uint32_t * instance_id);


/*
 * Start the reaper
 *
 * @param instance      - rwvcs instance
 * @param instance_name - instance name of the owning RWVM
 * @return              - RW_STATUS_SUCCESS
 *                        aborts on failure
 */
rw_status_t reaper_start(
    rwvcs_instance_ptr_t instance,
    const char * instance_name);

/*
 * Start a collection
 *
 * @param rwmain      - rwmain instance
 * @param parent_id   - instance name of parent component
 * @param pb          - protobuf collection definition
 * @param action      - action that triggered the collection start
 * @param inst_name_p - on success the instance_name of the started component
 * @return            - RW_STATUS_SUCCESS
 *                      aborts on failure
 */
static rw_status_t rwcollection_start(
    struct rwmain_gi * rwmain,
    const char * parent_id,
    vcs_manifest_collection * pb,
    vcs_manifest_action * m_action,
    char * instance_name,
    uint32_t instance_id);

/*
 * Annex a VM.  This is a developer utility that makes the current process take
 * over the specified VM.  From that point forward, this process will behave as
 * if it was that VM.
 *
 * The targeted VM, if it exists, is not currently alerted.
 *
 * @param instance    - rwvcs instance
 * @param m_action    - action definition that triggered this call
 * @return            - RW_STATUS_SUCCESS
 *                      aborts on failure
 */
static rw_status_t rwvm_annex(
    rwvcs_instance_ptr_t instance,
    vcs_manifest_action *m_action);

/*
 * Start a VM
 *
 * @param rwmain      - rwmain instance
 * @param parent_id   - instance name of parent component
 * @param m_rwvm      - vm definition
 * @param m_action    - action that triggered this call
 * @param inst_name_p - on success the instance_name of the started component
 * @return            - RW_STATUS_SUCCESS
 *                      aborts on failure
 */
static rw_status_t rwvm_start(
    struct rwmain_gi * rwmain,
    const char * parent_id,
    vcs_manifest_vm *m_rwvm,
    vcs_manifest_action *m_action,
    char * instance_name,
    uint32_t instance_id);

/*
 * Start a process
 *
 * @param instance    - rwmain instance
 * @param parent_id   - instance name of parent component
 * @param m_rwproc    - process definition
 * @param m_action    - action that triggered this event
 * @param inst_name_p - on success the instance_name of the started component
 * @return            - RW_STATUS_SUCCESS
 *                      aborts on failure
 */
static rw_status_t rwproc_start(
    struct rwmain_gi * rwmain,
    const char * parent_id,
    vcs_manifest_proc *m_rwproc,
    vcs_manifest_action *m_action,
    char * instance_name,
    uint32_t instance_id);

/*
 * Start a native process
 *
 * @param rwmain      - rwmain instance
 * @param parent_id   - instance name of parent component
 * @param m_proc      - process definition
 * @param m_action    - action that triggered this event
 * @param inst_name_p - on success the instance_name of the started component
 * @return            - RW_STATUS_SUCCESS
 *                      aborts on failure
 */
static rw_status_t rwproc_native_start(
    struct rwmain_gi * rwmain,
    const char * parent_id,
    vcs_manifest_native_proc *m_proc,
    vcs_manifest_action *m_action,
    char * instance_name,
    uint32_t instance_id);

/*
 * Get a VM from a pool of available VMs.
 *
 * @param rwvcs       - rwvcs instance
 * @param pool_name   - name of the VM pool
 * @param vm_ip_addr  - on success, the ip address of an available VM
 * @return            - RW_STATUS_SUCCESS
 *                      aborts on failure
 */
static rw_status_t rwvcs_get_vm_from_pool(
    rwvcs_instance_ptr_t rwvcs,
    char * pool_name,
    char * vm_ip_addr);

/*
 * Start a tasklet
 *
 * @param instance    - rwvcs instance
 * @param parent_id   - instance name of parent component
 * @param m_rwtasklet - tasklet definition
 * @param m_action    - action that triggered this event
 * @param inst_name_p - on success the instance_name of the started component
 * @return            - RW_STATUS_SUCCESS
 *                      aborts on failure
 */
static rw_status_t rwtasklet_start(
    struct rwmain_gi * rwmain,
    const char * parent_id,
    vcs_manifest_tasklet *m_rwtasklet,
    vcs_manifest_action *m_action,
    char * instance_name,
    uint32_t instance_id);

/*
 * Callback executed by lead VM's when there is a SERF event reported.
 *
 * @param ctx     - voidy rwvcs instance
 * @param event   - name of event being reported
 * @param member  - member being updated.  By convention member names are instance names.
 */
static void on_serf_event(
    void * ctx,
    const char * event,
    const struct serf_member * member);

/*
 * Wrapper around getenv that returns a newly allocated string containing
 * "KEY=VALUE" if the environemnt variable is set, otherwise just an empty
 * string.
 *
 * @param key   - name of the environment variable to lookup
 * @return      - a new string, "key=getenv(key)" if key is set in the
 *                environment otherwise an empty string.
 */
static char * envstr(const char * name);

/*
 * Fork a child process.  The process will be opened with a pipe which is
 * monitored by the runloop (rwmain->procs).  This allows the parent to know
 * when the child has exited and to act accordingly.
 *
 * @param rwmain          - rwmain instance
 * @param instance_name   - instance name of the process to start
 * @param path            - path to the executable
 * @param argv            - NULL-terminated list of arguments to pass to
 *                          the executable.  By convention the first argument
 *                          should be the path to the executable.
 * @param envp            - NULL-terminated list of environment variables to
 *                          append to current environment.  If NULL then only
 *                          the current environment will be used.
 * @param pid             - If not NULL, on success, the pid of the child process.
 * @parm reap             - True if this process should be put in the reaper's
 *                          process group.
 * @param track           - True if this process should be tracked via an open
 *                          file handle.
 * @param is_rwproc       - True if this process is an RWPROC being launched.
 * @param stream_paths    - Paths for stream redirection.
 * @param term_mode       - Specifies the terminal control mode
 * @return                - RW_STATUS_SUCCESS, RW_STATUS_FAILURE.
 */
static rw_status_t launch_process(
    struct rwmain_gi * rwmain,
    const char * instance_name,
    const char * path,
    char * const * argv,
    char * const * envp,
    bool reap,
    bool track,
    bool is_rwproc,
    const struct stream_paths * stream_paths,
    pid_t * pid,
    native_proc_term_mode_t term_mode);
#define reap_launched_process true
#define no_reap_launched_process false
#define track_launched_process true
#define no_track_launched_process false

/*
 * Create a new argv list with valgrind prepended
 *
 * @param argv            - program and argv to be valgrinded, NULL terminated
 * @param valgrind_args   - valgrind arguments of length n_valgrind_args
 * @param n_valgrind_args - number of valgrind args, used for convenience as
 *                          this is the format we get from the yang toolchain
 * @param prefix          - if specified, NULL terminated list of args to preceed
 *                          valgrind
 * @return                - newly allocated, NULL terminated list combining
 *                          argv and valgrind_argv
 */
static char ** add_valgrind(
    const char ** argv,
    const char ** valgrind_args,
    size_t n_valgrind_args,
    const char ** prefix);

/*
 * Given the master pty fd, open the slave fd and return the file descriptor.
 *
 * @param master_pty_fd   File descriptor of the master side of PTY
 *
 * @returns Returns RW_STATUS_SUCCESS on success, otherwise returns
 * RW_STATUS_FAILURE.
 */
static rw_status_t open_slave_pty(struct rwmain_gi * rwmain, int master_pty_fd);

/*
 * Starts a TerminalIO tasklet
 *
 * @param[in] rwmain     rwmain structure
 * @param[in] parent_id  id of the parent (here the VM)
 *
 * @returns Returns RW_STATUS_SUCCESS on success, otherwise returns
 * RW_STATUS_FAILURE.
 */
static rw_status_t start_terminal_io_tasklet(
    struct rwmain_gi * rwmain,
    const char* parent_id);

static rw_status_t event_run(
    struct rwmain_gi * rwmain,
    const char * parent_id,
    vcs_manifest_event *m_event)
{
  rw_status_t status;
  int i;
  vcs_manifest_action *m_action;
  char name[1024];


  for (i = 0 ; i < m_event->n_action ; i++) {
    m_action = m_event->action[i];
    RW_ASSERT(m_action);

    if (m_action->python_loop_expression
        && m_action->python_loop_iterate
        && m_action->python_loop_expression[0] != '\0'
        && m_action->python_loop_iterate[0] != '\0') {
      status = rwvcs_evaluate_python_loop_variables(
          rwmain->rwvx->rwvcs,
          m_action->python_loop_expression);
      RW_ASSERT(status == RW_STATUS_SUCCESS);

      while (true) {
        status = rwvcs_evaluate_python_loop_variables(
            rwmain->rwvx->rwvcs,
            m_action->python_loop_iterate);
        if (status != RW_STATUS_SUCCESS)
          break;

        status = rwvcs_variable_evaluate_str(
            rwmain->rwvx->rwvcs,
            m_action->name,
            name,
            sizeof(name));
        RW_ASSERT(status == RW_STATUS_SUCCESS);

        rwmain_trace_info(
            rwmain,
            "running action '%s' %d out of %ld",
            name,
            i,
            m_event->n_action);

        status = rwmain_action_run(rwmain, parent_id, m_action);
        if (status != RW_STATUS_SUCCESS) {
          rwmain_trace_info(rwmain, "Action run failure %s", name);
          return status;
        }
      }
    } else {
      rwmain_trace_info(
          rwmain,
          "running action '%s' %d out of %ld",
          m_action->name,
          i,
          m_event->n_action);
      status = rwmain_action_run(rwmain, parent_id, m_action);
      if (status != RW_STATUS_SUCCESS) {
        return status;
      }
    }
  }

  return RW_STATUS_SUCCESS;
}

static int rwmain_check_and_set_instance_name (
    struct rwmain_gi * rwmain,
    const char *orig_parent_id,
    const char *chk_instance_name,
    const vcs_manifest_action *action,
    char **instance_name,
    uint32_t *instance_id)
{
  RW_ASSERT(instance_name && instance_id);
  if ((*instance_name) && (*instance_id)) {
    goto done;
  }
  if (chk_instance_name) {
    rw_component_info cinfo;
    rw_status_t status = rwvcs_rwzk_lookup_component(
        rwmain->rwvx->rwvcs,
        chk_instance_name,
        &cinfo);
    if (status != RW_STATUS_SUCCESS) {
      goto done;
    }
    if (!strcmp(cinfo.component_name, action->start->component_name)) {
      rw_component_info chinfo;
      status = rwvcs_rwzk_lookup_component(
          rwmain->rwvx->rwvcs,
          cinfo.instance_name, 
          &chinfo);
      if ((status == RW_STATUS_SUCCESS)
          && (chinfo.state == RW_BASE_STATE_TYPE_TO_RECOVER)) {
        (*instance_name) = strdup(chinfo.instance_name);
        (*instance_id) = chinfo.instance_id;
        goto done;
      }
    }
    int indx;
    for (indx=0; !(*instance_name) && (indx < cinfo.n_rwcomponent_children); indx++) {
      if (!strcmp(orig_parent_id, cinfo.rwcomponent_children[indx])) {
        continue;
      }
      if (((*instance_id) = rwmain_check_and_set_instance_name(rwmain,
                                             orig_parent_id,
                                             cinfo.rwcomponent_children[indx],
                                             action,
                                             instance_name,
                                             instance_id))) {
        goto done;
      }
    }
  }
done:
  return (*instance_id);
}

rw_status_t rwmain_action_run(
    struct rwmain_gi * rwmain,
    const char * parent_id,
    vcs_manifest_action *action)
{
  rw_status_t status = RW_STATUS_FAILURE;
  vcs_manifest_component *m_component;
  char component_name[1024];

  if (action->annex) {
    // Lookup the name of the component to annex
    status = rwvcs_manifest_component_lookup(rwmain->rwvx->rwvcs, action->annex->component_name, &m_component);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    // Call the annex routine corresponding to the type of the component
    if (m_component->component_type == RWVCS_TYPES_COMPONENT_TYPE_RWVM)
      status = rwvm_annex(rwmain->rwvx->rwvcs, action);
    else
      RW_CRASH();
  } else if (action->start) {
    // Evaluate the variable list for the variables defined within the start action
    status = rwvcs_variable_list_evaluate(
        rwmain->rwvx->rwvcs,
        action->start->n_python_variable,
        action->start->python_variable);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    // Evaluate the component name
    status = rwvcs_variable_evaluate_str(
        rwmain->rwvx->rwvcs,
        action->start->component_name,
        component_name,
        sizeof(component_name));
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    // Lookup the name of the component to start
    status = rwvcs_manifest_component_lookup(rwmain->rwvx->rwvcs, component_name, &m_component);
    if (status == RW_STATUS_NOTFOUND)
      return status;

    char *instance_name = NULL;
    uint32_t instance_id = 0;
    rwmain_check_and_set_instance_name (rwmain, parent_id, parent_id, action, &instance_name, &instance_id);

    if (!instance_name) {
      status = generate_instance_name_id(rwmain->rwvx->rwvcs, action->start, &instance_name, &instance_id);
      RW_ASSERT(status == RW_STATUS_SUCCESS);
    }
    if (status != RW_STATUS_SUCCESS) {
      rwmain_trace_crit(
          rwmain,
          "Failed to get instance name and id for %s, parent %s",
          action->start->component_name,
          parent_id);
      RW_CRASH();
      goto done;
    }

    if (status == RW_STATUS_SUCCESS) {
      rw_status_t rs = rwvcs_instance_update_child_state(
        rwmain,
        instance_name,
        parent_id,
        m_component->component_type,
        RW_BASE_ADMIN_COMMAND_MAX_VALUE);
      RW_ASSERT(rs == RW_STATUS_SUCCESS);
    }

    switch (m_component->component_type) {
      case RWVCS_TYPES_COMPONENT_TYPE_RWCOLLECTION:
        status = rwcollection_start(rwmain, parent_id, m_component->rwcollection, action, instance_name, instance_id);
        break;

      case RWVCS_TYPES_COMPONENT_TYPE_RWVM:
        RWVCS_LATENCY_CHK_PRE(rwmain->rwvx->rwsched);
        status = rwvm_start(rwmain, parent_id, m_component->rwvm, action, instance_name, instance_id);
        RWVCS_LATENCY_CHK_POST(rwmain->rwvx->rwtrace, RWTRACE_CATEGORY_RWVCS,
                               rwvm_start, "rwvm_start:%s", component_name);
        break;

      case RWVCS_TYPES_COMPONENT_TYPE_RWPROC:
        RWVCS_LATENCY_CHK_PRE(rwmain->rwvx->rwsched);
        status = rwproc_start(rwmain, parent_id, m_component->rwproc, action, instance_name, instance_id);
        RWVCS_LATENCY_CHK_POST(rwmain->rwvx->rwtrace, RWTRACE_CATEGORY_RWVCS,
                               rwproc_start, "rwproc_start:%s", component_name);
        break;

      case RWVCS_TYPES_COMPONENT_TYPE_PROC:
      {
        char *rift_var_root = rwtasklet_info_get_rift_var_root((rwtasklet_info_t *)rwmain->tasklet_info);
        RW_ASSERT(rift_var_root);

        status = rw_setenv("RIFT_VAR_ROOT", rift_var_root);
        RW_ASSERT(status == RW_STATUS_SUCCESS);

        if (!strcmp(component_name, "RW.Redis.Server")) {
          /* generate the appropriate conf file and start the native proc using
           * the generated conf file */
          status = rwtasklet_create_rift_var_root(rift_var_root);
          RW_ASSERT(status == RW_STATUS_SUCCESS);

          status = rwmain_gen_redis_conf_file(rwmain, m_component);
          RW_ASSERT(status == RW_STATUS_SUCCESS);
          status = rwproc_native_start(rwmain, parent_id, m_component->native_proc, action, instance_name, instance_id);
          sleep(3);
          rwmain_start_redis_client(rwmain);
        } else {
          status = rwproc_native_start(rwmain, parent_id, m_component->native_proc, action, instance_name, instance_id);
        }

        break;
      }

      case RWVCS_TYPES_COMPONENT_TYPE_RWTASKLET:
        RWVCS_LATENCY_CHK_PRE(rwmain->rwvx->rwsched);
#ifdef RWVCS_NO_STBY_UAGENT_SPRT
        if (!action->start->mode_active && strstr(instance_name, "RW.uAgent")) {
          NEW_DBG_PRINTS("Standby TASKLET skipped -- %s --now\n", instance_name);
        }
        else 
#endif
        {
          status = rwtasklet_start(rwmain, parent_id, m_component->rwtasklet, action, instance_name, instance_id);
        }
        RWVCS_LATENCY_CHK_POST(rwmain->rwvx->rwtrace, RWTRACE_CATEGORY_RWVCS,
                               rwtasklet_start, "rwtasklet_start:%s", component_name);
        break;

      default:
        RW_CRASH();
    }

    if (status == RW_STATUS_SUCCESS
        //&& m_component->component_type != RWVCS_TYPES_COMPONENT_TYPE_RWCOLLECTION
       ) {
      rw_status_t rs = rwvcs_instance_update_child_state(
        rwmain,
        instance_name,
        parent_id,
        m_component->component_type,
        RW_BASE_ADMIN_COMMAND_MAX_VALUE);
      RW_ASSERT(rs == RW_STATUS_SUCCESS);
    }

    if (instance_name)
      free(instance_name);

  } else if (action->sleep) {
    rwmain_trace_info(
        rwmain,
        "Sleeping for %lf\n seconds",
        action->sleep->wait_time_usec / 1000.0 / 1000.0);
    usleep(action->sleep->wait_time_usec);
    status = RW_STATUS_SUCCESS;
  }

  if (status != RW_STATUS_SUCCESS)
    RW_CRASH();

done:
  return status;
}

static rw_status_t
generate_instance_name_id(
    rwvcs_instance_ptr_t rwvcs,
    vcs_manifest_action_start * start,
    char ** instance_name,
    uint32_t * instance_id)
{
  rw_status_t status;

  *instance_name = NULL;
  *instance_id = 0;

  if (start->instance_id) {
    int id;

    status = rwvcs_variable_evaluate_int(rwvcs, start->instance_id, &id);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    *instance_name = to_instance_name(start->component_name, id);
    RW_ASSERT(*instance_name);

    *instance_id = (uint32_t)id;
  } else {
    status = rwvcs_rwzk_next_instance_id(rwvcs, instance_id, NULL);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    *instance_name = to_instance_name(start->component_name, *instance_id);
    RW_ASSERT(*instance_name);
  }

  return RW_STATUS_SUCCESS;
}

rw_status_t reaper_start(rwvcs_instance_ptr_t instance, const char * instance_name)
{
  pid_t pid;
  int rc;
  char * reaper_sock_path;

  // Fork and exec the rwvcs-reaper
  RWTRACE_DEBUG(
      instance->rwvx->rwtrace,
      RWTRACE_CATEGORY_RWVCS,
      "Starting the rwvcs-reaper");

  rc = asprintf(&reaper_sock_path, "/tmp/reaper-%s", instance_name);
  RW_ASSERT(rc != -1);

  rc = unlink(reaper_sock_path);
  if ((rc == -1) && (errno != ENOENT)) {
    RWTRACE_ERROR(instance->rwvx->rwtrace,
                 RWTRACE_CATEGORY_RWVCS,
                 "unlink failed: program=%s errno=%s\n",
                 reaper_sock_path,
                 strerror(errno));
  }

  pid = fork();
  if (pid < 0) {
    RW_CRASH();
  }

  if (pid == 0) {
    char * argv[8];
    int argc = 0;

    char ld_lib_path[1024];
    snprintf(ld_lib_path, sizeof(ld_lib_path) - 1, 
        "LD_LIBRARY_PATH=%s/usr/lib:%s", getenv("RIFT_INSTALL"), getenv("LD_LIBRARY_PATH"));

    if (!getenv("RIFT_NO_SUDO_REAPER")) {
      argv[argc++] = "/usr/bin/sudo";
      argv[argc++] = "--non-interactive";
      argv[argc++] = ld_lib_path;
    }

    argv[argc++] = instance->reaper_path;
    argv[argc++] = "--socket_path";
    argv[argc++] = reaper_sock_path;
    argv[argc++] = "--kill_all";
    argv[argc] = NULL;
    // Close all the fds starting from 3
    closefrom(3);

    // Exec the new image
    RWTRACE_INFO(instance->rwvx->rwtrace,
                 RWTRACE_CATEGORY_RWVCS,
                 "calling execv for program: %s\n",
                 instance->reaper_path);

    execv(argv[0], argv);
    
    RWTRACE_ERROR(instance->rwvx->rwtrace,
                 RWTRACE_CATEGORY_RWVCS,
                 "execv failed: pwd=%s program=%s errno=%s\n",
                 get_current_dir_name(),
                 instance->reaper_path,
			           strerror(errno));

    RW_CRASH();
  } else {
    struct timeval timeout = {RWVCS_RWZK_TIMEOUT_S, 0};

    instance->reaper_sock = reaper_client_connect(reaper_sock_path, &timeout);
    if (instance->reaper_sock < 0) {
      RWTRACE_CRIT(
          instance->rwvx->rwtrace,
          RWTRACE_CATEGORY_RWVCS,
          "failed to connect to reaper socket at %s: %s",
          reaper_sock_path,
          strerror(-1 * instance->reaper_sock));
#if 1
      {
        FILE *in;
        extern FILE *popen();
        char buff[512];

        sprintf(buff, "stat %s", reaper_sock_path);

        /* popen creates a pipe so we can read the output
        of the program we are invoking */
        if ((in = popen(buff, "r"))) {
          fprintf(stderr, "Output: %s\n", buff);
          /* read the output, one line at a time */
          while (fgets(buff, sizeof(buff), in) != NULL ) {
            fprintf(stderr, "Output: %s", buff);
          }
          /* close the pipe */
          pclose(in);
        }
      }
#endif
      RW_CRASH();
    }
  }

  free(reaper_sock_path);

  return RW_STATUS_SUCCESS;
}

static rw_status_t rwcollection_start(
    struct rwmain_gi * rwmain,
    const char * parent_id,
    vcs_manifest_collection * pb,
    vcs_manifest_action * m_action,
    char * instance_name,
    uint32_t instance_id)
{
  rw_status_t status = RW_STATUS_SUCCESS;
  rw_component_info * component = NULL;
  vcs_manifest_event * m_event = NULL;
  rwvcs_instance_ptr_t rwvcs;

  rwvcs = rwmain->rwvx->rwvcs;

  if (rwvcs_rwzk_node_exists(rwvcs, instance_name)) {
    status = RW_STATUS_SUCCESS;
    if (m_action->start->has_config_ready &&
        m_action->start->config_ready) {
      status = rwvcs_rwzk_update_config_ready(rwvcs, instance_name, m_action->start->config_ready);
      RW_ASSERT(status == RW_STATUS_SUCCESS);
    }
    if (m_action->start->has_recovery_action &&
        m_action->start->recovery_action) {
      status = rwvcs_rwzk_update_recovery_action(rwvcs, instance_name, m_action->start->recovery_action);
      RW_ASSERT(status == RW_STATUS_SUCCESS);
    }
    goto done;
  }

  rwmain_trace_info(rwmain, "start rwcollection %s", instance_name);

  component = rwvcs_rwcollection_alloc(
      rwvcs,
      parent_id,
      m_action->start->component_name,
      instance_id,
      instance_name);
  if (!component) {
    RW_ASSERT(component);
    status = RW_STATUS_FAILURE;
    goto done;
  }

  component->has_state = true;
  component->state = RW_BASE_STATE_TYPE_STARTING;
  component->has_config_ready = m_action->start->has_config_ready;
  component->config_ready = m_action->start->config_ready;
  component->has_recovery_action = m_action->start->has_recovery_action;
  component->recovery_action = m_action->start->recovery_action;
  component->has_data_storetype = m_action->start->has_data_storetype;
  component->data_storetype = m_action->start->data_storetype;
  component->collection_info->collection_type = strdup(pb->collection_type);
  if (!component->collection_info->collection_type) {
    RW_ASSERT(component->collection_info->collection_type);
    goto done;
  }

  // Write the component info into the rwzk.
  status = rwvcs_rwzk_node_update(rwvcs, component);
  if (status != RW_STATUS_SUCCESS)
    goto done;

  /* Process onentry events */
  status = rwvcs_manifest_event_lookup("onentry", pb->event_list, &m_event);
  if (status == RW_STATUS_SUCCESS) {
    status = event_run(rwmain, component->instance_name, m_event);
    if (status != RW_STATUS_SUCCESS)
      goto done;
  }
  if (!strcmp(rwmain->parent_id, instance_name)) {
    rw_component_info info;
    rwvcs_instance_ptr_t rwvcs= rwmain->rwvx->rwvcs;
    status = rwvcs_rwzk_lookup_component(rwvcs, rwvcs->instance_name, &info);
    RW_ASSERT(status == RW_STATUS_SUCCESS);
    if (!info.rwcomponent_parent) {
      status = rwvcs_rwzk_update_parent(rwvcs, rwvcs->instance_name, rwmain->parent_id);
      RW_ASSERT(status == RW_STATUS_SUCCESS);
    }
    if (!strcmp(parent_id, rwvcs->instance_name)) {
      status = rwvcs_rwzk_update_parent(rwvcs, instance_name, NULL);
      RW_ASSERT(status == RW_STATUS_SUCCESS);
    }
  }

  // Update state in rwzk
  status = rwvcs_rwzk_update_state(
      rwvcs,
      component->instance_name,
      RW_BASE_STATE_TYPE_RUNNING);

  rwmain_dts_register_vcs_instance(rwmain, instance_name);

done:
  if (status != RW_STATUS_SUCCESS) {
    rwvcs_rwzk_update_state(
        rwvcs,
        component->instance_name,
        RW_BASE_STATE_TYPE_CRASHED);
    rwmain_trace_crit(
        rwmain,
        "Failed to start collection %s, %d",
        m_action->start->component_name,
        status);
    if (instance_name)
      free(instance_name);
  }

  if (component)
    protobuf_free(component);

  return status;
}

rw_status_t rwvm_annex(
    rwvcs_instance_ptr_t rwvcs,
    vcs_manifest_action *m_action)
{
  rw_status_t status;
  int instance_id;

  // Validate input parameters
  RW_CF_TYPE_VALIDATE(rwvcs, rwvcs_instance_ptr_t);
  RW_ASSERT(m_action);

  // Evaluate the rwvcs id
  status = rwvcs_variable_evaluate_int(
      rwvcs,
      m_action->annex->instance_id,
      &instance_id);
  RW_ASSERT(status == RW_STATUS_SUCCESS);
  RW_ASSERT(instance_id > 0);

  RWTRACE_INFO(rwvcs->rwvx->rwtrace,
      RWTRACE_CATEGORY_RWVCS,
      "annex rwvm instance-id = %d\n",
      instance_id);

  // Save the instance id of the RWVM into the identity structure
  rwvcs->identity.rwvm_instance_id = instance_id;
  RWTRACE_INFO(rwvcs->rwvx->rwtrace,
      RWTRACE_CATEGORY_RWVCS,
      "my rwvm_instance_id = %d\n",
      rwvcs->identity.rwvm_instance_id);

  // The operation was successful
  return RW_STATUS_SUCCESS;
}

static rw_status_t
rwvcs_get_vm_from_pool(
    rwvcs_instance_ptr_t rwvcs,
    char *pool_name,
    char *vm_ip_addr)
{
  rwvcs_vmpool_entry_t *vm_pool;
  rwvcs_vmip_entry_t *vm_ip_entry;
  rw_status_t status;
  rw_ip_addr_t tmp_ip_addr;

  RW_CF_TYPE_VALIDATE(rwvcs, rwvcs_instance_ptr_t);
  RW_ASSERT(pool_name);

  status = RW_SKLIST_LOOKUP_BY_KEY(
      &(rwvcs->vmpool_slist),
      pool_name,
      (void *)&vm_pool);
  RW_ASSERT(status == RW_STATUS_SUCCESS);

  switch (vm_pool->pool_type) {
    case RWCAL_VMPOOL_STATIC_LIST:
      vm_ip_entry = RW_SKLIST_HEAD(&(vm_pool->u.vmpool_ip_slist), rwvcs_vmip_entry_t);
      RW_ASSERT(vm_ip_entry);

      status = RW_SKLIST_REMOVE_BY_KEY(
          &(vm_pool->u.vmpool_ip_slist),
          vm_ip_entry->vmname,
          (void *)&vm_ip_entry);
      RW_ASSERT(vm_ip_entry);

      rw_ip_addr_t_get_str(vm_ip_addr, &(vm_ip_entry->vm_ip_addr));
      RW_FREE(vm_ip_entry);
      break;

    case RWCAL_VMPOOL_STATIC_RANGE:
      //TBD this needs to change when VMs can be freed back into the pool
      RW_ASSERT((vm_pool->u.vm_range.low_addr.u.v4.addr + vm_pool->u.vm_range.curr_index ) <= vm_pool->u.vm_range.high_addr.u.v4.addr);
      tmp_ip_addr = vm_pool->u.vm_range.low_addr;
      tmp_ip_addr.u.v4.addr += vm_pool->u.vm_range.curr_index;
      vm_pool->u.vm_range.curr_index += 1;
      rw_ip_addr_t_get_str(vm_ip_addr, &(tmp_ip_addr));
      break;

    default:
      RW_CRASH();
      break;
  }

  RWTRACE_INFO(rwvcs->rwvx->rwtrace,
      RWTRACE_CATEGORY_RWVCS,
      "Allocated VM IP ADDRESS = %s",
      vm_ip_addr);

  return RW_STATUS_SUCCESS;
}

const char *multivm_rwzk_path = "/sys/rwmain/iamup";
const char *multivm_rwzk_lock = "/sys/rwmain/iamup-LOCK";
static void watcher_multivm_rwmain(void* ud);

#define IS_MGMT_VM(rwvcs) ((rwvcs)->mgmt_info.state == RWVCS_TYPES_VM_STATE_MGMTACTIVE || (rwvcs)->mgmt_info.state == RWVCS_TYPES_VM_STATE_MGMTSTANDBY)
#define MGMT_VM_STATE(rwvcs) (rwvcs)->mgmt_info.state

static rw_status_t rwvm_start(
    struct rwmain_gi * rwmain,
    const char * parent_id,
    vcs_manifest_vm *m_rwvm,
    vcs_manifest_action *m_action,
    char *instance_name,
    uint32_t instance_id)
{
  int r;
  rw_status_t status;
  uint8_t uid;
  rw_component_info * rwvm;
  char vm_ip_address[256];
  rwvcs_instance_ptr_t rwvcs;

  char * riftinstall;
  char * rwmain_path;
  char * instance_arg;

  rwvcs = rwmain->rwvx->rwvcs;

  RWTRACE_INFO(rwvcs->rwvx->rwtrace,
      RWTRACE_CATEGORY_RWVCS,
      "VM = %s\n",
      instance_name);

  /*
   * The first VM started has the lovely role of needing to start up the container that
   * contains it.  That container will probably have a start action for the VM itself.
   * Check for that instance here and reset the parent/child relationship.
   */
  if (instance_id == rwvcs->identity.rwvm_instance_id) {
    rw_component_info info;
    char * current_instance = NULL;

    status = rwvcs_rwzk_update_parent(rwvcs, instance_name, parent_id);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    if (m_action->start->has_config_ready &&
        m_action->start->config_ready) {
      status = rwvcs_rwzk_update_config_ready(rwvcs, instance_name, m_action->start->config_ready);
      RW_ASSERT(status == RW_STATUS_SUCCESS);
    }

    if (m_action->start->has_recovery_action &&
        m_action->start->recovery_action) {
      status = rwvcs_rwzk_update_recovery_action(rwvcs, instance_name, m_action->start->recovery_action);
      RW_ASSERT(status == RW_STATUS_SUCCESS);
    }

    if (parent_id
        && !rwmain->parent_id) {
      rwmain->parent_id = strdup(parent_id);
      if (!rwmain->parent_id) {
        RW_ASSERT(0);
        goto done;
      }
    }

    RWTRACE_CRIT(rwvcs->rwvx->rwtrace,
        RWTRACE_CATEGORY_RWVCS,
        "VM = %s parent_id = %s rwvcs_rwzk_watcher_start(%s)\n",
        instance_name,
        rwmain->parent_id,
        multivm_rwzk_path);

    rwvcs_zk_closure_ptr_t closure = rwvcs_rwzk_watcher_start(
        rwvcs,
        multivm_rwzk_path, 
        rwmain->rwvx->rwsched_tasklet,
        rwsched_dispatch_get_main_queue(rwmain->rwvx->rwsched),
        watcher_multivm_rwmain,
        (void *)rwmain);
    RW_ASSERT(closure);

    status = rwvcs_rwzk_seed_auto_instance(rwvcs, 1, multivm_rwzk_lock);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    if (parent_id) {
      current_instance = strdup(parent_id);
      RW_ASSERT(current_instance);

      while (true) {
        status = rwvcs_rwzk_lookup_component(rwvcs, current_instance, &info);
        RW_ASSERT(status == RW_STATUS_SUCCESS);

        if (!info.rwcomponent_parent)
          break;

        if (!strncmp(instance_name, info.rwcomponent_parent, strlen(instance_name))) {
          status = rwvcs_rwzk_update_parent(rwvcs, current_instance, NULL);
          RW_ASSERT(status == RW_STATUS_SUCCESS);
          protobuf_free_stack(info);
          free(current_instance);
          break;
        }

        free(current_instance);
        current_instance = strdup(info.rwcomponent_parent);
        RW_ASSERT(current_instance);

        protobuf_free_stack(info);
      }
    }
    struct timespec tp;
    r = clock_gettime(CLOCK_MONOTONIC, &tp);
    RW_ASSERT(r == 0);
    rwmain->start_sec = tp.tv_sec;

    goto done;
  }

  // Trace the rwvm start operation
  RWTRACE_INFO(rwvcs->rwvx->rwtrace,
      RWTRACE_CATEGORY_RWVCS,
      "starting rwvm instance name = \"%s\"",
      instance_name);
  rw_component_info vm;
  bool alloced = true;
  status = rwvcs_rwzk_lookup_component(rwvcs, instance_name, &vm);
  if ((status == RW_STATUS_SUCCESS)
      && (vm.state == RW_BASE_STATE_TYPE_TO_RECOVER)) {
    rwvm = &vm;
    alloced = false;
  }
  else {
    // Create an rwvm structure
    rwvm = rwvcs_rwvm_alloc(
        rwvcs,
        parent_id,
        m_action->start->component_name,
        instance_id,
        instance_name);
  }
  RW_ASSERT(rwvm);


  /* If the VMs are collapsed, the ip address doesn't matter and we don't need to
   * require it in the manifest.  Just default to localhost
   */
  if (rwvcs->pb_rwmanifest->init_phase->settings->rwvcs->collapse_each_rwvm) {
    snprintf(vm_ip_address, 256, "127.%u.%u.1", instance_id / 256, instance_id % 256);
  } else {
    status = rwvcs_variable_evaluate_str(
        rwvcs,
        "$vm_ip_address",
        vm_ip_address,
        sizeof(vm_ip_address));
    RW_ASSERT(status == RW_STATUS_SUCCESS);
  }

  // Initialize the component info
  rwvm->vm_info->vm_ip_address = strdup(vm_ip_address);
  rwvm->has_state = true;
  if (alloced) { 
    rwvm->state = RW_BASE_STATE_TYPE_STARTING;
  }
  rwvm->has_config_ready = m_action->start->has_config_ready;
  rwvm->config_ready = m_action->start->config_ready;
  rwvm->has_recovery_action = m_action->start->has_recovery_action;
  rwvm->recovery_action = m_action->start->recovery_action;
  rwvm->has_data_storetype = m_action->start->has_data_storetype;
  rwvm->data_storetype = m_action->start->data_storetype;
  if (m_rwvm && m_rwvm->has_leader) {
    rwvm->vm_info->has_leader = true;
    rwvm->vm_info->leader = m_rwvm->leader;
  }

  riftinstall = getenv("RIFT_INSTALL");

  status = rw_instance_uid(&uid);
  RW_ASSERT(status == RW_STATUS_SUCCESS);

  // TODO:  this needs to be tucked away in rwlib
  r = asprintf(
      &rwmain_path,
      "%s/usr/bin/rwmain",
      riftinstall ? riftinstall : "");
  RW_ASSERT(r != -1);

  r = asprintf(&instance_arg, "%d", instance_id);
  RW_ASSERT(r != -1);

  if (m_rwvm && m_rwvm->pool_name) {
    status = rwvcs_get_vm_from_pool(rwvcs, m_rwvm->pool_name, vm_ip_address);
    RW_ASSERT(status == RW_STATUS_SUCCESS);
  }

  if (rwvcs->pb_rwmanifest->init_phase->settings->rwvcs->collapse_each_rwvm) {
    const char * argv[] = {
      rwmain_path,
      "--manifest", rwvcs->rwmanifest_xmlfile,
      "--name", m_action->start->component_name,
      "--instance", instance_arg,
      "--type", "rwvm",
      "--parent", parent_id,
      "--ip_address", vm_ip_address,
      NULL
    };

    char *rift_vm = getenv("RIFT_VAR_VM");
    char rift_var_vm[255];
    if (!rift_vm) {
      if (rwvcs->identity.rwvm_name) {
        snprintf(rift_var_vm, 255, "%s%c%s", rwvcs->pb_rwmanifest->bootstrap_phase->test_name,
                 '-', rwvcs->identity.rwvm_name);
      } else {
        snprintf(rift_var_vm, 255, "%s", rwvcs->pb_rwmanifest->bootstrap_phase->test_name);
      }
      rift_vm = &rift_var_vm[0];
    }

    if (rwvcs->pb_rwmanifest->init_phase->settings->rwvcs->collapse_each_rwprocess) {
      rwvm->vm_info->pid = getpid();

      status = rwvcs_rwzk_node_update(rwvcs, rwvm);
      RW_ASSERT(status == RW_STATUS_SUCCESS);

      setenv("RIFT_VAR_VM", rift_vm, true); 

      char *rift_var_root = rwtasklet_info_get_rift_var_root((rwtasklet_info_t *)rwmain->tasklet_info);
      RW_ASSERT(rift_var_root);
      setenv("RIFT_VAR_ROOT", rift_var_root, true); 
      status = rwtasklet_create_rift_var_root(rift_var_root);  
      RW_ASSERT(status == RW_STATUS_SUCCESS);

      rwvcs->rwmain_f(sizeof(argv)/sizeof(argv[0])-1, (char **)argv, rwvcs->envp);
    } else {
      struct timeval timeout = { .tv_sec = RWVCS_RWZK_TIMEOUT_S, .tv_usec = 0 };
      char * envp[7] = {NULL, NULL, NULL,NULL, NULL, NULL, NULL};
      pid_t pid;

      r = asprintf(&envp[0], "RIFT_INSTANCE_UID=%u", uid);
      RW_ASSERT(r != -1);

      r = asprintf(&envp[1], "RIFT_INSTANCE_REAL_UID=%u", getuid());
      RW_ASSERT(r != -1);

      r = asprintf(&envp[2], "RIFT_VAR_VM=%s", rift_vm); 
      RW_ASSERT(r != -1);

      char *rift_var_root = rwtasklet_info_get_rift_var_root((rwtasklet_info_t *)rwmain->tasklet_info);
      RW_ASSERT(rift_var_root);
      status = rwtasklet_create_rift_var_root(rift_var_root);
      RW_ASSERT(status == RW_STATUS_SUCCESS);
      r = asprintf(&envp[3], "RIFT_VAR_ROOT=%s", rift_var_root);
      RW_ASSERT(r != -1);

      if (rwvcs->ld_preload) {
        r = asprintf(&envp[4], "LD_PRELOAD=%s", rwvcs->ld_preload);
        RW_ASSERT(r != -1);
      }


      // Lock so that the parent can initialize the zk data before the child updates it
      status = rwvcs_rwzk_lock(rwvcs, instance_name, &timeout);
      RW_ASSERT(status == RW_STATUS_SUCCESS);

      if (m_rwvm->valgrind && m_rwvm->valgrind->has_enable && m_rwvm->valgrind->enable) {
        char ** valgrind_argv;

        valgrind_argv = add_valgrind(
            argv,
            (const char **)m_rwvm->valgrind->opts,
            m_rwvm->valgrind->n_opts,
            NULL);
        RW_ASSERT(valgrind_argv);

        status = launch_process(
            rwmain,
            instance_name,
            valgrind_argv[0],
            valgrind_argv,
            (char **)envp,
            no_reap_launched_process,
            no_track_launched_process,
            false,
            NULL,
            &pid,
            NATIVE_PROC_TERM_MODE_NONE);

        for (size_t i = 0; valgrind_argv[i]; ++i)
          free(valgrind_argv[i]);
        free(valgrind_argv);
      } else {
        status = launch_process(
            rwmain,
            instance_name,
            argv[0],
            (char **)argv,
            (char **)envp,
            no_reap_launched_process,
            no_track_launched_process,
            false,
            NULL,
            &pid,
            NATIVE_PROC_TERM_MODE_NONE);
      }

      if (status != RW_STATUS_SUCCESS) {
        rwvm->state = RW_BASE_STATE_TYPE_CRASHED;
        rwmain_trace_crit(
            rwmain,
            "Failed to launch %s, %d",
            instance_name,
            status);
      }
      else {
        rwvm->vm_info->has_pid = true;
        rwvm->vm_info->pid = pid;
      }

      status = rwvcs_rwzk_node_update(rwvcs, rwvm);
      RW_ASSERT(status == RW_STATUS_SUCCESS);

      status = rwvcs_rwzk_unlock(rwvcs, instance_name);
      RW_ASSERT(status == RW_STATUS_SUCCESS);

      free(envp[0]);
      free(envp[1]);
      if (envp[2])
        free(envp[2]);
    }
  } else {
    char * uid_str = NULL;
    char * real_uid_str = NULL;
    char * ld_preload = NULL;
    char * install_dir;
    char * ssh_wrapper;
    char * rift_shell;
    char * rsync_cmd;
    pid_t pid;
    struct timeval timeout = { .tv_sec = RWVCS_RWZK_TIMEOUT_S, .tv_usec = 0 };

    // TODO:  Tuck this away in a shared location
    install_dir = getenv("INSTALLDIR");

    r = asprintf(&uid_str, "RIFT_INSTANCE_UID=%u", uid);
    RW_ASSERT(r != -1);

    r = asprintf(&real_uid_str, "RIFT_INSTANCE_REAL_UID=%u", getuid());
    RW_ASSERT(r != -1);

    if (rwvcs->ld_preload) {
      r = asprintf(&ld_preload, "LD_PRELOAD=%s", rwvcs->ld_preload);
      RW_ASSERT(r != -1);
    }

    r = asprintf(&ssh_wrapper, "%s/usr/bin/rwssh_wrapper", install_dir ? install_dir : "");
    RW_ASSERT(r != -1);

    r = asprintf(&rift_shell, "%s/../rift-shell", install_dir ? install_dir : "");
    RW_ASSERT(r != -1);


    // Sync the manifest file on the remote side.  Using rsync ensures that we don't actually
    // do anything if the files are the same (shared filesystem).
    r = asprintf(&rsync_cmd, "/usr/bin/rsync %s %s:%s",
        rwvcs->rwmanifest_xmlfile,
        vm_ip_address,
        rwvcs->rwmanifest_xmlfile);
    RW_ASSERT(r != -1);
    r = system(rsync_cmd);

    if (r != 0) {
      RWTRACE_ERROR(
          rwvcs->rwvx->rwtrace,
          RWTRACE_CATEGORY_RWVCS,
          "rsync %s %s:%s failed (%d)",
          rwvcs->rwmanifest_xmlfile,
          vm_ip_address,
          rwvcs->rwmanifest_xmlfile,
          r);
    }
    //FIXME - Ignore for now
    //RW_ASSERT(r == 0); // In case of localhost, I'm sseing this failure.

    // Lock so that the parent can initialize the zk data before the child updates it
    status = rwvcs_rwzk_lock(rwvcs, instance_name, &timeout);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

#if 1
    char *rwmain_screenrc;
    r = asprintf(
        &rwmain_screenrc,
        "%s/etc/rwmain.screenrc",
        riftinstall ? riftinstall : "");
    RW_ASSERT(r != -1);

    if (m_rwvm->valgrind
        && m_rwvm->valgrind->has_enable
        && m_rwvm->valgrind->enable) {
      char ** valgrind_argv;
      const char * prefix[] = {
        "ssh", "-n", vm_ip_address, "-o", "StrictHostKeyChecking=no",
//"/usr/bin/screen", "-d",  "-m", "-c", rwmain_screenrc,
        ssh_wrapper, uid_str, real_uid_str, ld_preload ? ld_preload : "",
        rift_shell,
          "--use-existing",
          "--",
          NULL };
      const char * argv[] = {
        rwmain_path,
          "--manifest", rwvcs->rwmanifest_xmlfile,
          "--name", m_action->start->component_name,
          "--instance", instance_arg,
          "--type", "rwvm",
          "--parent", parent_id,
          "--ip_address", vm_ip_address,
        NULL };


      valgrind_argv = add_valgrind(
          argv,
          (const char **)m_rwvm->valgrind->opts,
          m_rwvm->valgrind->n_opts,
          prefix);
      RW_ASSERT(valgrind_argv);

      status = launch_process(
          rwmain,
          instance_name,
          valgrind_argv[0],
          valgrind_argv,
          NULL,
          no_reap_launched_process,
          no_track_launched_process,
          false,
          NULL,
          &pid,
          NATIVE_PROC_TERM_MODE_NONE);

      for (size_t i = 0; valgrind_argv[i]; ++i)
        free(valgrind_argv[i]);
      free(valgrind_argv);
    } else {
      // ssh will will call $SHELL -c which drops one layer of quotes.
      const char * argv[] = {
        "ssh", "-n", vm_ip_address, "-o", "StrictHostKeyChecking=no",
        //"/usr/bin/screen", "-d", "-m", "-c", rwmain_screenrc,
        ssh_wrapper, uid_str, real_uid_str, ld_preload ? ld_preload : "",
        rift_shell,
          "--use-existing",
          "--",
        rwmain_path,
          "--manifest", rwvcs->rwmanifest_xmlfile,
          "--name", m_action->start->component_name,
          "--instance", instance_arg,
          "--type", "rwvm",
          "--parent", parent_id,
          "--ip_address", vm_ip_address,
        NULL
      };

      status = launch_process(
        rwmain,
        instance_name,
        argv[0],
        (char **)argv,
        NULL,
        no_reap_launched_process,
        no_track_launched_process,
        false,
        NULL,
        &pid,
        NATIVE_PROC_TERM_MODE_NONE);
    }

#else
    pid = 0;
    status = RW_STATUS_SUCCESS;
#endif
    if (status == RW_STATUS_SUCCESS)
    {
      rwvm->vm_info->has_lpid = true;
      rwvm->vm_info->lpid = pid;
    } else {
      rwvm->state = RW_BASE_STATE_TYPE_CRASHED;
      rwmain_trace_crit(
          rwmain,
          "Failed to launch %s, %d",
          instance_name,
          status);
    }

    status = rwvcs_rwzk_node_update(rwvcs, rwvm);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    status = rwvcs_rwzk_unlock(rwvcs, instance_name);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    free(uid_str);
    free(real_uid_str);
    if (ld_preload)
      free(ld_preload);
    free(ssh_wrapper);
    free(rift_shell);
    free(rsync_cmd);
    free(rwmain_screenrc);
  }

  free(rwmain_path);
  free(instance_arg);
  if (alloced) {
    free(rwvm);
  }

  status = RW_STATUS_SUCCESS;

done:
  if (status != RW_STATUS_SUCCESS) {
    if (instance_name)
      free(instance_name);
  }

  // The operation was successful
  return status;
}

static rw_status_t rwproc_start(
    struct rwmain_gi * rwmain,
    const char * parent_id,
    vcs_manifest_proc *m_rwproc,
    vcs_manifest_action *m_action,
    char * instance_name,
    uint32_t instance_id)
{
  int r;
  rw_status_t status;
  rw_component_info * rwproc;
  int argc = 16;
  char * riftinstall;
  int argi = 0;
  char ** argv;
  rwvcs_instance_ptr_t rwvcs;
  char * ld_library_path = NULL;
  char * python_path = NULL;
  char * instance_arg = NULL;
  char * vm_instance_arg = NULL;

  rwvcs = rwmain->rwvx->rwvcs;
  RW_CF_TYPE_VALIDATE(rwvcs, rwvcs_instance_ptr_t);

  RWTRACE_INFO(rwvcs->rwvx->rwtrace,
      RWTRACE_CATEGORY_RWVCS,
      "starting rwproc instance name = \"%s\"",
      instance_name);

  rw_component_info proc;
  bool alloced = true;
  status = rwvcs_rwzk_lookup_component(rwvcs, instance_name, &proc);
  if ((status == RW_STATUS_SUCCESS) 
      && (proc.state == RW_BASE_STATE_TYPE_TO_RECOVER)) {
    rwproc = &proc;
    alloced = false;
  }
  else {
    rwproc = rwvcs_rwproc_alloc(
        rwvcs,
        parent_id,
        m_action->start->component_name,
        instance_id,
        instance_name);
  }
  RW_ASSERT(rwproc);

  if (m_rwproc && m_rwproc->run_as && !rwvcs->pb_rwmanifest->init_phase->settings->rwvcs->collapse_each_rwprocess)
    argc += 9;

  argv = (char **)malloc(argc * sizeof(char *));
  RW_ASSERT(argv);
  bzero(argv, argc * sizeof(char *));

  char *grp = NULL;
  if (m_rwproc && m_rwproc->run_as && !rwvcs->pb_rwmanifest->init_phase->settings->rwvcs->collapse_each_rwprocess) {
    asprintf (&grp, "#%d", getpgrp());
    RW_ASSERT(grp);

    argv[argi++] = "sudo";
    argv[argi++] = "-g";
    argv[argi++] = grp;
    argv[argi++] = "--non-interactive";
    argv[argi++] = "--preserve-env";
    argv[argi++] = "--user";
    argv[argi++] = m_rwproc->run_as;

    // RIFT-5353 - We need to set some *_PATH variables on the command line as they
    // get reset by the linker before sudo gets the change to interpet them.
    ld_library_path = envstr("LD_LIBRARY_PATH");
    python_path = envstr("PYTHONPATH");
    argv[argi++] = ld_library_path;
    argv[argi++] = python_path;
  }

  r = asprintf(&vm_instance_arg, "%d", rwvcs->identity.rwvm_instance_id);
  RW_ASSERT(r != -1);

  r = asprintf(&instance_arg, "%d", instance_id);
  RW_ASSERT(r != -1);


  // TODO:  this needs to be tucked away in rwlib
  riftinstall = getenv("RIFT_INSTALL");
  r = asprintf(
      &argv[argi++],
      "%s/usr/bin/rwmain",
      riftinstall ? riftinstall : "");
  RW_ASSERT(r != -1);

  argv[argi++] = "--manifest";
  argv[argi++] = rwvcs->rwmanifest_xmlfile;


  argv[argi++] = "--name";
  argv[argi++] = m_action->start->component_name;

  argv[argi++] = "--vm_instance";
  argv[argi++] = vm_instance_arg;

  argv[argi++] = "--instance";
  argv[argi++] = instance_arg;

  argv[argi++] = "--type";
  argv[argi++] = "rwproc";

  argv[argi++] = "--parent";
  argv[argi++] = (char *)parent_id;

  argv[argi++] = "--ip_address";
  argv[argi++] = rwmain->vm_ip_address;

  RW_ASSERT(argi == argc - 1);

  rwproc->proc_info->has_pid = true;

  rwproc->has_state = true;
  if (rwproc->state != RW_BASE_STATE_TYPE_TO_RECOVER) {
    rwproc->state = RW_BASE_STATE_TYPE_STARTING;
  }
  rwproc->has_config_ready = m_action->start->has_config_ready;
  rwproc->config_ready = m_action->start->config_ready;
  rwproc->has_recovery_action = m_action->start->has_recovery_action;
  rwproc->recovery_action = m_action->start->recovery_action;
  rwproc->has_data_storetype = m_action->start->has_data_storetype;
  rwproc->data_storetype = m_action->start->data_storetype;

  // This needs to run first to make sure the mq is created prior to the
  // child writing to it.
  status = rwproc_heartbeat_subscribe(rwmain, instance_name);
  RW_ASSERT(status == RW_STATUS_SUCCESS);

  char *rift_vm = getenv("RIFT_VAR_VM");
  char rift_var_vm[255];
  if (!rift_vm) {
    if (rwvcs->identity.rwvm_name) {
      snprintf(rift_var_vm, 255, "%s%c%s", rwvcs->pb_rwmanifest->bootstrap_phase->test_name,
               '-', rwvcs->identity.rwvm_name);
    } else {
      snprintf(rift_var_vm, 255, "%s", rwvcs->pb_rwmanifest->bootstrap_phase->test_name);
    }
    rift_vm = &rift_var_vm[0];
  }

  if (rwvcs->pb_rwmanifest->init_phase->settings->rwvcs->collapse_each_rwprocess) {
    rwproc->proc_info->pid = getpid();

    status = rwvcs_rwzk_node_update(rwvcs, rwproc);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    setenv("RIFT_VAR_VM", rift_vm, true);

    char *rift_var_root = rwtasklet_info_get_rift_var_root((rwtasklet_info_t *)rwmain->tasklet_info);
    RW_ASSERT(rift_var_root);
    setenv("RIFT_VAR_ROOT", rift_var_root, true);
    status = rwtasklet_create_rift_var_root(rift_var_root);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    rwvcs->rwmain_f(argc-1, argv, rwvcs->envp);
  } else {
    uint8_t uid;
    char * uid_str;
    struct timeval timeout = { .tv_sec = RWVCS_RWZK_TIMEOUT_S, .tv_usec = 0 };
    char * envp[7] = {NULL, NULL, NULL,NULL, NULL, NULL, NULL};
    pid_t pid;

    status = rw_instance_uid(&uid);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    r = asprintf(&uid_str, "%u", uid);
    RW_ASSERT(r != -1);

    r = asprintf(&envp[0], "RIFT_INSTANCE_UID=%u", uid);
    RW_ASSERT(r != -1);

    r = asprintf(&envp[1], "RIFT_INSTANCE_REAL_UID=%u", getuid());
    RW_ASSERT(r != -1);

    r = asprintf(&envp[2], "RIFT_VAR_VM=%s", rift_vm);
    RW_ASSERT(r != -1);

    char *new_name = rwtasklet_info_get_rift_var_root((rwtasklet_info_t *)rwmain->tasklet_info);
    RW_ASSERT(new_name);
    status = rwtasklet_create_rift_var_root(new_name);
    RW_ASSERT(status == RW_STATUS_SUCCESS);
    r = asprintf(&envp[3], "RIFT_VAR_ROOT=%s", new_name);
    RW_ASSERT(r != -1);

    if (rwvcs->ld_preload) {
      r = asprintf(&envp[4], "LD_PRELOAD=%s", rwvcs->ld_preload);
      RW_ASSERT(r != -1);
    }


    // Lock so that the parent can initialize the zk data before the child updates it
    status = rwvcs_rwzk_lock(rwvcs, instance_name, &timeout);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    if (m_rwproc->valgrind
        && m_rwproc->valgrind->has_enable
        && m_rwproc->valgrind->enable) {
      char ** valgrind_argv;

      valgrind_argv = add_valgrind(
          (const char **)argv,
          (const char **)m_rwproc->valgrind->opts,
          m_rwproc->valgrind->n_opts,
          NULL);
      RW_ASSERT(valgrind_argv);

      status = launch_process(
          rwmain,
          instance_name,
          valgrind_argv[0],
          valgrind_argv,
          envp,
          no_reap_launched_process,
          track_launched_process,
          true, //is_rwproc = true
          NULL,
          &pid,
          NATIVE_PROC_TERM_MODE_NONE);

      for (size_t i = 0; valgrind_argv[i]; ++i)
        free(valgrind_argv[i]);
      free(valgrind_argv);
    } else {
      status = launch_process(
        rwmain,
        instance_name,
        argv[0],
        argv,
        envp,
        no_reap_launched_process,
        track_launched_process,
        true, //is_rwproc = true
        NULL,
        &pid,
        NATIVE_PROC_TERM_MODE_NONE);
    }

    if (status == RW_STATUS_SUCCESS)
      rwproc->proc_info->pid = pid;
    else {
      rwproc->state = RW_BASE_STATE_TYPE_CRASHED;
      rwmain_trace_crit(
          rwmain,
          "Failed to launch %s, %d",
          instance_name,
          status);
    }

    status = rwvcs_rwzk_node_update(rwvcs, rwproc);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    status = rwvcs_rwzk_unlock(rwvcs, instance_name);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    free(uid_str);
    free(envp[0]);
    free(envp[1]);
    if (envp[2])
      free(envp[2]);
  }

  free(argv);
  if (alloced) {
    free(rwproc);
  }
  else {
    protobuf_c_message_free_unpacked_usebody(NULL, &proc.base);
  }


  if (grp)
    free(grp);

  if (ld_library_path)
    free(ld_library_path);

  if (python_path)
    free(python_path);

  if (vm_instance_arg)
    free(vm_instance_arg);

  if (instance_arg)
    free(instance_arg);

  if (status != RW_STATUS_SUCCESS) {
    if (instance_name)
      free(instance_name);
  }
  return status;
}

static rw_status_t rwproc_native_start(
    struct rwmain_gi * rwmain,
    const char * parent_id,
    vcs_manifest_native_proc *m_proc,
    vcs_manifest_action *m_action,
    char * instance_name,
    uint32_t instance_id)
{
  rw_status_t status;
  rw_component_info * rwproc;
  rwvcs_instance_ptr_t rwvcs;

  char * saveptr = NULL;
  char * token = NULL;
  char * env_copy = NULL;
  char * argv_copy = NULL;

  size_t argc = 2;
  size_t argi = 0;
  char ** argv = NULL;
  char ** envp = NULL;
  pid_t pid = 0;
  int vcs_ns = 0;
  struct stream_paths stream_paths;
  native_proc_term_mode_t term_mode = NATIVE_PROC_TERM_MODE_NONE;

  rwvcs = rwmain->rwvx->rwvcs;

  if (m_proc->environment) {
    size_t i = 0;
    size_t env_len = 1;
    char * p = NULL;

    p = m_proc->environment;
    while ((p = strchr(p, ' ')) != NULL) {
      env_len++;
      while (*p == ' ')
        p++;
    }
    env_len++;

    if (m_proc->has_interactive) {
      // For RIFT_VM_INSTANCE_ID and few others
      env_len += 4;
    }

    envp = (char **)malloc(sizeof(char *) * env_len);
    RW_ASSERT(envp);
    bzero(envp, sizeof(char *) * env_len);

    env_copy = m_proc->environment;
    RW_ASSERT(env_copy);

    token = strtok_r(env_copy, " \t", &saveptr);
    while (token) {
      envp[i] = token;
      i++;
      token = strtok_r(NULL, " \t", &saveptr);
    }

    if (m_proc->has_interactive) {
      char *vm_instance_arg = NULL;
      asprintf(&vm_instance_arg, "RIFT_VM_INSTANCE_ID=%u",
               rwvcs->identity.rwvm_instance_id);
      envp[i] = vm_instance_arg;
      i++;
    }

    char *rift_vm = getenv("RIFT_VAR_VM");
    char rift_var_vm[255];
    if (!rift_vm) {
      if (rwvcs->identity.rwvm_name) {
        snprintf(rift_var_vm, 255, "%s%c%s", rwvcs->pb_rwmanifest->bootstrap_phase->test_name,
                 '-', rwvcs->identity.rwvm_name);
      } else {
        snprintf(rift_var_vm, 255, "%s", rwvcs->pb_rwmanifest->bootstrap_phase->test_name);
      }
      rift_vm = &rift_var_vm[0];
    }

    asprintf(&envp[i], "RIFT_VAR_VM=%s", rift_vm);
    i++;

    char *rift_var_root = rwtasklet_info_get_rift_var_root((rwtasklet_info_t *)rwmain->tasklet_info);
    RW_ASSERT(rift_var_root);
    status = rwtasklet_create_rift_var_root(rift_var_root);
    RW_ASSERT(status == RW_STATUS_SUCCESS);
    asprintf(&envp[i], "RIFT_VAR_ROOT=%s", rift_var_root);
    i++;

    envp[i] = NULL;
    saveptr = NULL;
  } else if (m_proc->has_interactive) {
    int overwrite = 1;
    char vm_instance_str[UINT_STR_SZ] = {0};
    sprintf(vm_instance_str, "%u", rwvcs->identity.rwvm_instance_id);
    setenv("RIFT_VM_INSTANCE_ID", vm_instance_str, overwrite);
  }

  if (m_proc->args) {
    char * p;

    p = m_proc->args;
    while ((p = strchr(p, ' ')) != NULL) {
      argc++;
      while (*p == ' ')
        p++;
    }
    argc++;
  }

  if (m_proc->run_as)
    argc += 9;

  argv = (char **)malloc(sizeof(char *) * argc);
  RW_ASSERT(argv);
  bzero(argv, sizeof(char *) * argc);

  char *grp = NULL;
  char *env_lib = NULL;
  char *env_py = NULL;
  char *exe_p = strdup(m_proc->exe_path);

  if (m_proc->run_as) {
    asprintf (&grp, "#%d", getpgrp());
    RW_ASSERT(grp);
    env_lib = envstr("LD_LIBRARY_PATH");
    env_py = envstr("PYTHONPATH");

    argv[argi++] = "sudo";
    argv[argi++] = "-g";
    argv[argi++] = grp;
    argv[argi++] = "--non-interactive";
    argv[argi++] = "--preserve-env";
    argv[argi++] = "--user";
    argv[argi++] = m_proc->run_as;

    // RIFT-5353 - We need to set some *_PATH variables on the command line as they
    // get reset by the linker before sudo gets the change to interpet them.

    argv[argi++] = env_lib;
    argv[argi++] = env_py;
  }

  argv[argi++] = exe_p;

  if (argc > 2) {
    argv_copy = strdup(m_proc->args);
    RW_ASSERT(argv_copy);

    token = strtok_r(argv_copy, " \t", &saveptr);
    while (token) {
      argv[argi++] = token;
      token = strtok_r(NULL, " \t", &saveptr);
      RW_ASSERT(argi < argc);
    }
  }

  if (m_proc->has_interactive) {
    term_mode = NATIVE_PROC_TERM_MODE_INTERACTIVE;

    // Launch a pseudo terminal tasklet required for interactive processes
    start_terminal_io_tasklet(rwmain, parent_id);
  }

  rw_component_info proc;
  bool alloced = true;
  status = rwvcs_rwzk_lookup_component(rwvcs, instance_name, &proc);
  if ((status == RW_STATUS_SUCCESS) 
      && (proc.state == RW_BASE_STATE_TYPE_TO_RECOVER)) {
    rwproc = &proc;
    alloced = false;
    if (rwproc->has_mode_active && !rwproc->mode_active) {
      rwproc->has_state = true;
      rwproc->state = RW_BASE_STATE_TYPE_RUNNING;
      rwproc->mode_active = true;
      int r = reaper_client_add_pid(rwvcs->reaper_sock, rwproc->proc_info->pid);
      if (r) {
        rwmain_trace_crit(
            rwmain,
            "Failed to send pid %d to reaper: %s",
            rwproc->proc_info->pid,
            strerror(r));
        RW_CRASH();
      }
      goto recovery_done;
    }
  }
  else {
    rwproc = rwvcs_proc_alloc(
        rwvcs,
        parent_id,
        m_action->start->component_name,
        instance_id,
        instance_name);
    RW_ASSERT(rwproc);
    rwproc->has_mode_active = m_action->start->has_mode_active;
    rwproc->mode_active = m_action->start->mode_active;
  }
  RW_ASSERT(rwproc);

  // Switch to the appropriate network namespace, if any.  RIFT-3034
  if (m_proc->network_namespace) {
    int r;

    RWNETNS_LOCK();

    vcs_ns = rwnetns_get_current_netfd();
    if (vcs_ns <= 0) {
      RWTRACE_CRIT(
          rwvcs->rwvx->rwtrace,
          RWTRACE_CATEGORY_RWVCS,
          "Failed to get current network namespace");
      status = RW_STATUS_FAILURE;
      RWNETNS_UNLOCK();
      goto done;
    }

    r = rwnetns_change(m_proc->network_namespace);
    if (r) {
      RWTRACE_CRIT(
          rwvcs->rwvx->rwtrace,
          RWTRACE_CATEGORY_RWVCS,
          "Failed to switch to network namespace '%s'",
          m_proc->network_namespace);
      status = RW_STATUS_FAILURE;
      RWNETNS_UNLOCK();
      goto done;
    }
  }

  stream_paths.stdout_path = m_proc->stdout;
  stream_paths.stderr_path = m_proc->stderr;

  if (m_proc->valgrind
      && m_proc->valgrind->has_enable
      && m_proc->valgrind->enable) {
    char ** valgrind_argv;

    valgrind_argv = add_valgrind(
        (const char **)argv,
        (const char **)m_proc->valgrind->opts,
        m_proc->valgrind->n_opts,
        NULL);
    RW_ASSERT(valgrind_argv);

    status = launch_process(
      rwmain,
      instance_name,
      valgrind_argv[0],
      valgrind_argv,
      envp,
      reap_launched_process,
      track_launched_process,
      false,
      &stream_paths,
      &pid,
      term_mode);
    for (size_t i = 0; valgrind_argv[i]; ++i)
      free(valgrind_argv[i]);
    free(valgrind_argv);
  } else {
    status = launch_process(
      rwmain,
      instance_name,
      argv[0],
      argv,
      envp,
      reap_launched_process,
      track_launched_process,
      false,
      &stream_paths,
      &pid,
      term_mode);
  }

  if (m_proc->network_namespace) {
    int r;

    r = rwnetns_change_fd(vcs_ns);
    if (r) {
      RWTRACE_CRIT(
          rwvcs->rwvx->rwtrace,
          RWTRACE_CATEGORY_RWVCS,
          "Failed to restore to network namespace from '%s'",
          m_proc->network_namespace);
      RW_CRASH();
    }

    close(vcs_ns);
    RWNETNS_UNLOCK();
  }

  rwproc->has_state = true;
  rwproc->state = status == RW_STATUS_SUCCESS ? RW_BASE_STATE_TYPE_RUNNING : RW_BASE_STATE_TYPE_CRASHED;

  if (rwproc->state == RW_BASE_STATE_TYPE_CRASHED) {
      rwmain_trace_crit(
          rwmain,
          "Failed to launch %s, %d",
          instance_name,
          status);
  }

  rwproc->proc_info->has_pid = true;
  rwproc->proc_info->pid = pid;
  rwproc->proc_info->has_native = true;
  rwproc->proc_info->native = true;
  rwproc->has_config_ready = m_action->start->has_config_ready;
  rwproc->config_ready = m_action->start->config_ready;
  rwproc->has_recovery_action = m_action->start->has_recovery_action;
  rwproc->recovery_action = m_action->start->recovery_action;
  rwproc->has_data_storetype = m_action->start->has_data_storetype;
  rwproc->data_storetype = m_action->start->data_storetype;
  rwproc->has_mode_active = m_action->start->has_mode_active;
  rwproc->mode_active = m_action->start->mode_active;

recovery_done:
  status = rwvcs_rwzk_node_update(rwvcs, rwproc);
  RW_ASSERT(status == RW_STATUS_SUCCESS);

done:
  if (grp) {
    RW_FREE(grp);
  }
  if (env_lib) {
    RW_FREE(env_lib);
  }
  if (env_py) {
    RW_FREE(env_py);
  }
  if (exe_p) {
    RW_FREE(exe_p);
  }

  if (argv)
    free(argv);

  if (envp)
    free(envp);

  if (env_copy)
    free(env_copy);

  if (argv_copy)
    free(argv_copy);

  if (status != RW_STATUS_SUCCESS) {
    if (instance_name)
      free(instance_name);
  }

  if (rwproc && alloced) {
    free(rwproc);
  }

  return status;
}

static rw_status_t rwtasklet_start(
    struct rwmain_gi * rwmain,
    const char * parent_id,
    vcs_manifest_tasklet *m_rwtasklet,
    vcs_manifest_action *m_action,
    char * instance_name,
    uint32_t instance_id)
{
  rw_status_t status;
  rw_component_info * component = NULL;
  struct rwmain_tasklet * rt = NULL;

  RWLOG_EVENT(rwmain->rwvx->rwlog, RwVcs_notif_VtaskletStart, instance_name, instance_id);

  rw_component_info tasklet;
  bool alloced = true;
  RWVCS_LATENCY_CHK_PRE(rwmain->rwvx->rwsched);
  status = rwvcs_rwzk_lookup_component(rwmain->rwvx->rwvcs, instance_name, &tasklet);
  if ((status == RW_STATUS_SUCCESS) 
      && (tasklet.state == RW_BASE_STATE_TYPE_TO_RECOVER)) {
    component = &tasklet;
    alloced = false;
  }
  else {
    component = rwvcs_rwtasklet_alloc(
        rwmain->rwvx->rwvcs,
        parent_id,
        m_action->start->component_name,
        instance_id,
        instance_name);
  }
  RWVCS_LATENCY_CHK_POST(rwmain->rwvx->rwtrace, RWTRACE_CATEGORY_RWVCS,
                         rwvcs_rwtasklet_alloc, "rwvcs_rwtasklet_alloc:%s", instance_name);
  if (!component) {
    rwmain_trace_crit(rwmain, "Failed to allocate component for %s", instance_name);
    status = RW_STATUS_FAILURE;
    RW_CRASH();
    goto done;
  }

  component->has_state = true;
  if (component->state != RW_BASE_STATE_TYPE_TO_RECOVER) {
    component->state = RW_BASE_STATE_TYPE_INITIALIZING;
  }
  component->has_config_ready = m_action->start->has_config_ready;
  component->config_ready = m_action->start->config_ready;
  component->has_recovery_action = m_action->start->has_recovery_action;
  component->recovery_action = m_action->start->recovery_action;
  component->has_data_storetype = m_action->start->has_data_storetype;
  component->data_storetype = m_action->start->data_storetype;
  component->has_mode_active = m_action->start->has_mode_active;
  component->mode_active = m_action->start->mode_active;
  RWVCS_LATENCY_CHK_PRE(rwmain->rwvx->rwsched);
  status = rwvcs_rwzk_node_update(rwmain->rwvx->rwvcs, component);
  RWVCS_LATENCY_CHK_POST(rwmain->rwvx->rwtrace, RWTRACE_CATEGORY_RWVCS,
                         rwvcs_rwzk_node_update,"rwvcs_rwzk_node_update: %s:%s for %s", 
                         m_rwtasklet->plugin_name, m_rwtasklet->plugin_directory, instance_name);

  RWVCS_LATENCY_CHK_PRE(rwmain->rwvx->rwsched);
  rwmain_tasklet_mode_active_t mode_active = {
    .has_mode_active = m_action->start->has_mode_active,
    .mode_active = m_action->start->mode_active
  };
  rt = rwmain_tasklet_alloc(
      instance_name,
      instance_id,
      &mode_active,
      m_rwtasklet->plugin_name,
      m_rwtasklet->plugin_directory,
      rwmain->rwvx->rwvcs);
  RWVCS_LATENCY_CHK_POST(rwmain->rwvx->rwtrace, RWTRACE_CATEGORY_RWVCS,
                         rwmain_tasklet_alloc,"rwmain_tasklet_alloc: %s:%s for %s", 
                         m_rwtasklet->plugin_name, m_rwtasklet->plugin_directory, instance_name);
  if (!rt) {
    rwmain_trace_crit(rwmain, "Failed to allocate rwmain_tasklet for %s", instance_name);
    status = RW_STATUS_FAILURE;
    RW_CRASH();
    goto done;
  }
  rt->rwmain = rwmain;
  rt->tasklet_info->data_store = m_action->start->data_storetype;
  if (rwmain->rwvx->rwvcs->pb_rwmanifest->init_phase->settings->rwvcs->collapse_each_rwvm) {
    rt->tasklet_info->vm_ip_address = RW_STRDUP("127.0.2.1");
  } else {
    rt->tasklet_info->vm_ip_address = RW_STRDUP(rwmain->vm_ip_address);
  }

  RWVCS_LATENCY_CHK_PRE(rwmain->rwvx->rwsched);
  status = rwmain_tasklet_start(rwmain, rt);
  RWVCS_LATENCY_CHK_POST(rwmain->rwvx->rwtrace, RWTRACE_CATEGORY_RWVCS,
                         rwmain_tasklet_start,"rwmain_tasklet_start:%s", instance_name);
  if (status != RW_STATUS_SUCCESS) {
    rwmain_trace_crit(rwmain, "Failed to start tasklet %s", instance_name);

    status = rwvcs_rwzk_update_state(
        rwmain->rwvx->rwvcs,
        instance_name,
        RW_BASE_STATE_TYPE_CRASHED);

    rwmain_trace_crit(
        rwmain,
        "Failed to  start tasklet %s, %d",
        instance_name,
        status);

    if (status != RW_STATUS_SUCCESS)
      rwmain_trace_crit(rwmain, "Failed to mark %s as CRASHED", instance_name);

    RW_CRASH();
    goto done;
  }

done:
  if (status != RW_STATUS_SUCCESS) {
    if (rt)
      rwmain_tasklet_free(rt);
    if (instance_name)
      free(instance_name);
  }
  if (alloced) {
    free(component);
  }
  else {
    protobuf_c_message_free_unpacked_usebody(NULL, &tasklet.base);
  }


  return status;
}

rw_status_t rwmain_tasklet_restart(
    struct rwmain_gi * rwmain,
    const char * tasklet_instance)
{
  rw_status_t status;
  struct rwmain_tasklet * rt = NULL;

  status = RW_SKLIST_LOOKUP_BY_KEY(&(rwmain->tasklets), &tasklet_instance, &rt);
  if (status != RW_STATUS_SUCCESS) {
    rwmain_trace_crit(
        rwmain,
        "Unknown tasklet %s: %d",
        tasklet_instance,
        status);
    return status;
  }

  /*
   * XXX:
   * It's an open question on if we should be doing module_init/deinit and
   * component_init/deinit and maybe even instance_alloc/free here.
   *
   * Not that it matters right now because no one is actually implementing any
   * of the deinit/free/stop functions anyways.
   */
  rt->plugin_interface->instance_stop(
      rt->plugin_klass,
      rt->h_component,
      rt->h_instance);

  rt->plugin_interface->instance_start(
      rt->plugin_klass,
      rt->h_component,
      rt->h_instance);

  return RW_STATUS_SUCCESS;
}

static void
stop_runloop(rwsched_CFRunLoopTimerRef timer, void *ctx)
{
  rwsched_instance_ptr_t rwsched = (rwsched_instance_ptr_t)ctx;

  RW_CF_TYPE_VALIDATE(rwsched, rwsched_instance_ptr_t);
  rwsched_instance_CFRunLoopStop(rwsched);
}

static void on_serf_event(void * ctx, const char * event, const struct serf_member * member)
{
  rw_status_t status;
  rwvcs_instance_ptr_t rwvcs;
  rw_component_info info;


  rwvcs = (rwvcs_instance_ptr_t)ctx;

  status = rwvcs_rwzk_lookup_component(rwvcs, member->name, &info);
  if (status != RW_STATUS_SUCCESS) {
    RWTRACE_ERROR(
        rwvcs->rwvx->rwtrace,
        RWTRACE_CATEGORY_RWVCS,
        "Serf event %s on non-existant member %s",
        event,
        member->name);
    exit(-1);
    return;
  }

  if (strncmp(member->status, "alive", 5)) {
    status = rwvcs_rwzk_update_state(rwvcs, member->name, RW_BASE_STATE_TYPE_LOST);
    RWTRACE_CRIT(
        rwvcs->rwvx->rwtrace,
        RWTRACE_CATEGORY_RWVCS,
        "Serf event %s on member %s - marking the RWVM as LOST",
        event,
        member->name);
    if (rwvcs->heartbeatmon_enabled) {
      exit(-1);
    }
  }

  protobuf_free_stack(info);
}

rw_status_t
rwmain_rwvm_init(
    struct rwmain_gi * rwmain,
    vcs_manifest_vm * vm_def,
    const char * component_name,
    uint32_t instance_id,
    const char * instance_name,
    const char * parent_id)
{
  rw_status_t status;
  rw_component_info * rwvm;
  vcs_manifest_event * m_event;
  rwvcs_instance_ptr_t rwvcs;

  rwvcs = rwmain->rwvx->rwvcs;

  rw_component_info vm;
  status = rwvcs_rwzk_lookup_component(rwvcs, instance_name, &vm);
  if ((status == RW_STATUS_SUCCESS) 
      && (vm.state == RW_BASE_STATE_TYPE_TO_RECOVER)) {
    status = rwvcs_rwzk_node_update(rwvcs, &vm);
    RW_ASSERT(status == RW_STATUS_SUCCESS);
  }
  else {
    // Called and then immediately freed as a side effect gets the linkage to the
    // Does not update parent here
    rwvm = rwvcs_rwvm_alloc(
        rwvcs,
        (rwvcs->mgmt_info.state != RWVCS_TYPES_VM_STATE_MGMTACTIVE)?parent_id:NULL,
        component_name,
        instance_id,
        instance_name);
    RW_ASSERT(rwvm);
    free(rwvm);
  }

  if (IS_MGMT_VM(rwvcs)) {
    rw_component_info rw_vm;
    status = rwvcs_rwzk_lookup_component(rwvcs, instance_name, &rw_vm);
    RW_ASSERT(status == RW_STATUS_SUCCESS);
    rw_vm.vm_info->has_vm_state = true;
    rw_vm.vm_info->vm_state = MGMT_VM_STATE(rwvcs);
    status = rwvcs_rwzk_node_update(rwvcs, &rw_vm);
    RW_ASSERT(status == RW_STATUS_SUCCESS);
  }

  status = reaper_start(rwvcs, instance_name);
  RW_ASSERT(status == RW_STATUS_SUCCESS);
  {
    int r = reaper_client_add_pid(rwvcs->reaper_sock, getpgrp());
    if (r) {
      rwmain_trace_crit(
          rwmain,
          "Failed to send vm pid %d to reaper: %s",
          getpid(),
          strerror(r));
      RW_CRASH();
    }
  }

  // Start the crash reporter.  RIFT-3392
  //
  // If this VM doesn't have a parent, it's being used to bootstrap and always
  // needs to start a crash reporter.  Otherwise, the VM only needs to start
  // the crash reporter if we are not collapsing each vm.
  if (!parent_id
      || !rwvcs->pb_rwmanifest->init_phase->settings->rwvcs->collapse_each_rwvm) {
    int r;
    char * install_dir;
    char * argv[4];

    install_dir = getenv("INSTALLDIR");

    r = asprintf(&argv[0], "%s/usr/bin/rwlogd-report-cores", install_dir ? install_dir : "");
    RW_ASSERT(r != -1);
    argv[1] = "-i";
    argv[2] = strdup(instance_name);
    argv[3] = NULL;

    status = launch_process(
        rwmain,
        "crash-reporter",
        argv[0],
        argv,
        NULL,
        reap_launched_process,
        track_launched_process,
        false,
        NULL,
        NULL,
        NATIVE_PROC_TERM_MODE_NONE);
    if (status != RW_STATUS_SUCCESS) {
      RWTRACE_CRIT(
          rwvcs->rwvx->rwtrace,
          RWTRACE_CATEGORY_RWVCS,
          "Failed to start crash-reporter");
    }

    free(argv[0]);
    free(argv[2]);
  }


  // Lookup the "onentry" event for the rwvm
  if (vm_def) {
    status = rwvcs_manifest_event_lookup("onentry", vm_def->event_list, &m_event);
    if (status == RW_STATUS_SUCCESS) {
      status = event_run(rwmain, instance_name, m_event);
      RW_ASSERT(status == RW_STATUS_SUCCESS);
    }
  }

  // Do not start if disabled in manifest
  if (!rwvcs->pb_rwmanifest->init_phase->settings->rwvcs->collapse_each_rwvm
      && rwvcs->pb_rwmanifest->bootstrap_phase->serf
      && rwvcs->pb_rwmanifest->bootstrap_phase->serf->has_start
      && rwvcs->pb_rwmanifest->bootstrap_phase->serf->start) {
    RW_ASSERT(rwvcs->pb_rwmanifest->bootstrap_phase->zookeeper);
    // Start heartbeat monitoring
    status = heartbeatmon_start(
        rwmain,
        rwmain->vm_ip_address,
        rwvcs->pb_rwmanifest->bootstrap_phase->zookeeper->master_ip,
        instance_name);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    if (!(vm_def && !vm_def->leader))
      rwmain->serf_handler = serf_register_event_callback(rwmain->serf, on_serf_event, rwvcs);
      rwvcs->heartbeatmon_enabled = true;
  }

  status = rwvcs_rwzk_update_state(rwvcs, instance_name, RW_BASE_STATE_TYPE_RUNNING);
  RW_ASSERT(status == RW_STATUS_SUCCESS);

  return RW_STATUS_SUCCESS;
}

static char * envstr(const char * name) {
  char * val;
  char * ret;

  val = getenv(name);
  if (!val) {
    ret = strdup("");
    RW_ASSERT(ret);
  } else {
    int r;

    r = asprintf(&ret, "%s=%s", name, val);
    RW_ASSERT(r != -1);
  }

  return ret;
}

rw_status_t
rwmain_rwproc_init(
    struct rwmain_gi * rwmain,
    vcs_manifest_proc * proc_def,
    const char * component_name,
    uint32_t instance_id,
    const char * instance_name,
    const char * parent_id)
{
  rw_status_t status;
  rw_component_info * rwproc;
  rwvcs_instance_ptr_t rwvcs;

  rwvcs = rwmain->rwvx->rwvcs;

  rw_component_info proc;
  bool alloced = true;
  RWVCS_LATENCY_CHK_PRE(rwmain->rwvx->rwsched);
  status = rwvcs_rwzk_lookup_component(rwvcs, instance_name, &proc);
  if ((status == RW_STATUS_SUCCESS) 
      && (proc.state == RW_BASE_STATE_TYPE_TO_RECOVER)) {
    rwproc = &proc;
    alloced = false;
    status = rwvcs_rwzk_node_update(rwvcs, rwproc);
    RW_ASSERT(status == RW_STATUS_SUCCESS);
  }
  else {
    rwproc = rwvcs_rwproc_alloc(
        rwvcs,
        parent_id,
        component_name,
        instance_id,
        instance_name);
  }
  RW_ASSERT(rwproc);
  RWVCS_LATENCY_CHK_POST(rwmain->rwvx->rwtrace, RWTRACE_CATEGORY_RWVCS,
                         rwvcs_rwproc_alloc, "rwvcs_rwproc_alloc:%s", instance_name);

  // Save our PID into the proc_info
  rwproc->proc_info->has_pid = true;
  rwproc->proc_info->pid = getpid();

  rwproc->has_state = true;
  rwproc->state = RW_BASE_STATE_TYPE_STARTING;
  rwproc->proc_info->has_native = true;
  rwproc->proc_info->native = false;

  if (!parent_id) {
    status = rwvcs_rwzk_node_update(rwvcs, rwproc);
    RW_ASSERT(status == RW_STATUS_SUCCESS);
  }

  // Start any of the components within the rwproc (these should be rwtasklets)
  //
  // TODO:  We should be directly looking up the components here, verifying they
  // are tasklets and then calling rwtasklet_start directly.
  for (size_t i = 0; proc_def && i < proc_def->n_tasklet; ++i) {
    vcs_manifest_action action;

    vcs_manifest_action__init(&action);

    action.name = strdup(proc_def->tasklet[i]->name);
    RW_ASSERT(action.name);

    action.start = (vcs_manifest_action_start *)malloc(sizeof(vcs_manifest_action_start));
    RW_ASSERT(action.start); vcs_manifest_action_start__init( action.start);

    action.start->component_name = strdup(proc_def->tasklet[i]->component_name);
    RW_ASSERT(action.start->component_name);

    if (proc_def->tasklet[i]->has_instance_id) {
      int r;

      r = asprintf(&action.start->instance_id, "%u", proc_def->tasklet[i]->instance_id);
      RW_ASSERT(r != -1);
    }

    int num_children = rwproc->n_rwcomponent_children;
    char *child_name = NULL;
    while (num_children 
           && (child_name = rwproc->rwcomponent_children[num_children - 1])) {
      if (strstr(child_name, action.start->component_name)) {
        rw_component_info child;
        status = rwvcs_rwzk_lookup_component(
            rwmain->rwvx->rwvcs,
            child_name, 
            &child);
        if ((status == RW_STATUS_SUCCESS)
            && (child.state == RW_BASE_STATE_TYPE_TO_RECOVER)) {
          int r = asprintf(&action.start->instance_id, "%u", (unsigned int)child.instance_id);
          RW_ASSERT(r != -1);
          break;
        }
      }
      num_children--;
    }
    action.start->n_python_variable = proc_def->tasklet[i]->n_python_variable;
    action.start->python_variable = proc_def->tasklet[i]->python_variable;
    action.start->has_config_ready = proc_def->tasklet[i]->has_config_ready;
    action.start->config_ready = proc_def->tasklet[i]->config_ready;
    action.start->has_recovery_action = proc_def->tasklet[i]->has_recovery_action;
    action.start->recovery_action = proc_def->tasklet[i]->recovery_action;
    action.start->has_data_storetype = proc_def->tasklet[i]->has_data_storetype;
    action.start->data_storetype = proc_def->tasklet[i]->data_storetype; 
    action.start->has_mode_active = proc_def->tasklet[i]->has_mode_active;
    action.start->mode_active = proc_def->tasklet[i]->mode_active;
     
    RWVCS_LATENCY_CHK_PRE(rwmain->rwvx->rwsched);                           
    status = rwmain_action_run(rwmain, rwproc->instance_name, &action);
    RW_ASSERT(status == RW_STATUS_SUCCESS);
    RWVCS_LATENCY_CHK_POST(rwmain->rwvx->rwtrace, RWTRACE_CATEGORY_RWVCS,
                           rwmain_action_run, "rwmain_action_run:%s", instance_name);

    action.start->n_python_variable = 0;
    action.start->python_variable = NULL;

    protobuf_c_message_free_unpacked_usebody(NULL, &action.base);
  }

  status = rwproc_heartbeat_publish(rwmain, instance_name);
  RW_ASSERT(status == RW_STATUS_SUCCESS);

  RWVCS_LATENCY_CHK_PRE(rwmain->rwvx->rwsched);                           
  status = rwvcs_rwzk_update_state(rwvcs, instance_name, RW_BASE_STATE_TYPE_RUNNING);
  RW_ASSERT(status == RW_STATUS_SUCCESS);
  RWVCS_LATENCY_CHK_POST(rwmain->rwvx->rwtrace, RWTRACE_CATEGORY_RWVCS,
                         rwvcs_rwzk_update_state, "rwvcs_rwzk_update_state:%s", instance_name);

  if (alloced) {
    free(rwproc);
  }
  else {
    protobuf_c_message_free_unpacked_usebody(NULL, &proc.base);
  }

  return RW_STATUS_SUCCESS;
}

rw_status_t
rwmain_stop_instance(
    struct rwmain_gi * rwmain,
    rw_component_info * id)
{
  rw_status_t status;
  bool halt_runloop = false;
  rwvcs_instance_ptr_t rwvcs;

  rwvcs = rwmain->rwvx->rwvcs;

  fprintf(stderr, "----------------------------STOPPING %s --------------\n", id->instance_name);

  RW_ASSERT(id->component_type == RWVCS_TYPES_COMPONENT_TYPE_RWTASKLET
      || id->component_type == RWVCS_TYPES_COMPONENT_TYPE_RWPROC
      || id->component_type == RWVCS_TYPES_COMPONENT_TYPE_PROC
      || id->component_type == RWVCS_TYPES_COMPONENT_TYPE_RWVM);

  if (id->component_type == RWVCS_TYPES_COMPONENT_TYPE_RWTASKLET) {
    struct rwmain_tasklet * rt = NULL;

    status = RW_SKLIST_REMOVE_BY_KEY(&(rwmain->tasklets), &id->instance_name, &rt);
    if (status == RW_STATUS_SUCCESS)
      rwmain_tasklet_free(rt);

    status = RW_STATUS_SUCCESS;
  } else if (id->component_type == RWVCS_TYPES_COMPONENT_TYPE_RWPROC) {
    if (id->proc_info->pid != getpid()) {
      RWTRACE_CRIT(rwmain->rwvx->rwtrace,
                   RWTRACE_CATEGORY_RWVCS,
                   "pid mismatch in vcs:%d/getpid:%d for %s", 
                   id->proc_info->pid, getpid(),
                   id->instance_name);
    }
    halt_runloop = !rwvcs->pb_rwmanifest->init_phase->settings->rwvcs->collapse_each_rwprocess;

    for (int i = 0; i < id->n_rwcomponent_children; ++i) {
      rw_component_info child;

      status = rwvcs_rwzk_lookup_component(rwvcs, id->rwcomponent_children[i], &child);
      if (status != RW_STATUS_SUCCESS) {
        rwmain_trace_error(
            rwmain,
            "Failed to lookup %s while stopping %s",
            id->rwcomponent_children[i],
            id->instance_name);
        continue;
      }

      rw_status_t rs = rwvcs_instance_update_child_state(
        rwmain,
        child.instance_name,
        id->instance_name,
        RWVCS_TYPES_COMPONENT_TYPE_MAX_VALUE, //==UNKNOWN
        RW_BASE_ADMIN_COMMAND_STOP);
      RW_ASSERT(rs == RW_STATUS_SUCCESS);

      status = rwmain_stop_instance(rwmain, &child);
      RW_ASSERT(status == RW_STATUS_SUCCESS);

      protobuf_free_stack(child);
    }
  } else if (id->component_type == RWVCS_TYPES_COMPONENT_TYPE_PROC) {
    struct rwmain_proc * rp;

    status = RW_SKLIST_REMOVE_BY_KEY(&(rwmain->procs), &id->instance_name, &rp);
    if (status == RW_STATUS_SUCCESS)
      rwmain_proc_free(rp);
    else
      RW_CRASH();
  } else if (id->component_type == RWVCS_TYPES_COMPONENT_TYPE_RWVM) {
      halt_runloop = !(
          rwvcs->pb_rwmanifest->init_phase->settings->rwvcs->collapse_each_rwvm
          && rwvcs->pb_rwmanifest->init_phase->settings->rwvcs->collapse_each_rwprocess);

      if (id->vm_info->has_leader && id->vm_info->leader) {
        rw_component_info parent;
        status = rwvcs_rwzk_lookup_component(rwvcs, id->rwcomponent_parent, &parent);
        RW_ASSERT(status == RW_STATUS_SUCCESS);
        if (parent.component_type == RWVCS_TYPES_COMPONENT_TYPE_RWCOLLECTION) {
          rwvcs_component_delete(rwvcs, &parent);
        }
        protobuf_free_stack(parent);
      }
      RW_ASSERT(id->vm_info->pid == getpid());
  }

  if (halt_runloop) {
    /* Add a timer to the main cfrunloop that will exit in 5 seconds.  The delay is to allow
     * for any final messages to be sent out and for the zookeeper to get updated.
     */

    rwsched_CFRunLoopTimerContext cf_context = { 0, NULL, NULL, NULL, NULL };
    double timer_interval = 5.0;
    rwsched_CFRunLoopRef runloop;
    rwsched_CFRunLoopTimerRef cftimer;

    runloop = rwsched_tasklet_CFRunLoopGetCurrent(rwvcs->rwvx->rwsched_tasklet);
    cf_context.info = rwvcs->rwvx->rwsched;
    cftimer = rwsched_tasklet_CFRunLoopTimerCreate(
        rwvcs->rwvx->rwsched_tasklet,
        kCFAllocatorDefault,
        CFAbsoluteTimeGetCurrent() + timer_interval,
        timer_interval,
        0,
        0,
        stop_runloop,
        &cf_context);

    RW_CF_TYPE_VALIDATE(cftimer, rwsched_CFRunLoopTimerRef);
    rwsched_tasklet_CFRunLoopAddTimer(
        rwvcs->rwvx->rwsched_tasklet,
        runloop,
        cftimer,
        rwvcs->rwvx->rwsched->main_cfrunloop_mode);
  }

  rwvcs_component_delete(rwvcs, id);

  return RW_STATUS_SUCCESS;
}

// Heartbeat monitoring (SERF for now)
static rw_status_t heartbeatmon_start(
    struct rwmain_gi * rwmain,
    char * vm_ip_address,
    char * lead_vm_ip_address,
    const char * instance_name)
{
  int rc;
  pid_t pid;
  rw_status_t status;
  rwvcs_instance_ptr_t rwvcs;
  char * node = NULL;
  char * bindaddr = NULL;
  char * rpcaddr = NULL;
  char * joinaddr = NULL;

  rwvcs = rwmain->rwvx->rwvcs;

  RW_ASSERT(lead_vm_ip_address);

  // serf -node
  rc = asprintf(&node, "-node=%s", instance_name);
  RW_ASSERT(rc != -1);

  // serf bind address and port -bind
  rc = asprintf(&bindaddr, "-bind=%s:%d",vm_ip_address, SERF_BIND_PORT);
  RW_ASSERT(rc != -1);

  // serf rpc addr and port -rpc-addr
  rc = asprintf(&rpcaddr, "-rpc-addr=%s:%d", vm_ip_address, SERF_RPC_PORT);
  RW_ASSERT(rc != -1);

  // serf join addr and port - which is bind address of the lead VM -join
  rc = asprintf(&joinaddr, "-retry-join=%s:%d", lead_vm_ip_address, SERF_BIND_PORT);
  RW_ASSERT(rc != -1);

  char * argv[] = {
    "/usr/bin/serf",
    "agent",
    "-encrypt=1FzgH8LsTtr0Wopn4934OQ==",
    "-log-level=err",
    node,
    bindaddr,
    rpcaddr,
    joinaddr,
    "-retry-max", "30",
    "-retry-interval", "1s",
    NULL
  };

  status = launch_process(
      rwmain,
      "serf",
      argv[0],
      argv,
      NULL,
      reap_launched_process,
      track_launched_process,
      false,
      NULL,
      &pid,
      NATIVE_PROC_TERM_MODE_NONE);
  if (status == RW_STATUS_SUCCESS) {
    rwmain->serf = serf_instance_alloc(rwvcs, vm_ip_address, SERF_RPC_PORT);
    RW_ASSERT(rwmain->serf);

    serf_rpc_connect(rwmain->serf, 1.0, 0);
  }

  return status;
}

static rw_status_t launch_process(
    struct rwmain_gi * rwmain,
    const char * instance_name,
    const char * path,
    char * const * argv,
    char * const * envp,
    bool reap,
    bool track,
    bool is_rwproc,
    const struct stream_paths * stream_paths,
    pid_t * pid,
    native_proc_term_mode_t term_mode)
{
  int r;
  int pipe_fds[2];
  pid_t lpid;
  char ** env = NULL;
  rwvcs_instance_ptr_t rwvcs;

  rwvcs = rwmain->rwvx->rwvcs;

  RW_ASSERT(rwvcs->reaper_sock > 0);

  if (envp != NULL) {
    size_t i = 0;
    size_t env_len = 1;

    if (rwvcs->envp) {
      for (; rwvcs->envp[env_len]; env_len++) {;}
    }

    for (i = 0; envp[i]; env_len++, i++) {;}

    env_len++;

    env = (char **)malloc(sizeof(char *) * env_len);
    bzero(env, sizeof(char *) * env_len);
    i = 0;
    if (rwvcs->envp) {
      for (i = 0; rwvcs->envp[i]; ++i)
        env[i] = rwvcs->envp[i];
    }

    for (int j = 0; envp[j] && i < env_len; ++i, ++j)
      env[i] = envp[j];

    RW_ASSERT(i < env_len);
  }

  r = pipe(pipe_fds);
  if (r) {
    int err = errno;
    rwmain_trace_error(rwmain, "pipe: %s", strerror(err));
    return RW_STATUS_FAILURE;
  }

  lpid = fork();
  if (lpid < 0) {
    int err = errno;
    fprintf(stderr, "fork gone wrong(%s): %s", path, strerror(err));
    rwmain_trace_error(rwmain, "fork: %s", strerror(err));
    return RW_STATUS_FAILURE;
  }

  if (lpid == 0) {
    const int term_size = 256;
    char stdin_term_name[term_size];
    char term_name[term_size];
    bool found_pty = false;

    ttyname_r(STDIN_FILENO, stdin_term_name, term_size);

    /*
     * Do this as early as possible in case the parent exits.
     *
     * Note that this will not work to terminate the child if the
     * exec'ed application calls setpgid()
     */
    r = prctl(PR_SET_PDEATHSIG, SIGTERM);
    RW_ASSERT(r == 0);

    /*
     * This is easier then trying to track down all the places we open
     * a new file and storing that.  The only thing we need to leave open
     * is the stdout, stderr, stdio and the write side of the pipe.
     */
    int last_fd = (int) lastfd();
    for (int fd = 3; fd <= last_fd; fd++) {
      if (term_mode == NATIVE_PROC_TERM_MODE_INTERACTIVE && isatty(fd)) {
        ttyname_r(fd, term_name, term_size);
        if (strncmp(stdin_term_name, term_name, term_size) != 0) {
          /* 
           * Is a tty and has a different term_name, must be master pty. Create
           * a slave from master for the child process
           */
          open_slave_pty(rwmain, fd);
          found_pty = true;
        } else {
          close(fd);
        }
      } else if (fd != pipe_fds[1]) {
        close(fd);
      }
    }

    if (term_mode == NATIVE_PROC_TERM_MODE_INTERACTIVE && !found_pty) {
      // No Pseudo terminal found, don't launch the process
      rwmain_trace_crit(rwmain, "No pseudo terminal found. Failed to launch the process");
      RW_ASSERT(found_pty);
    }

    if (reap) {
      /* Attached to pseudo terminal, process already a session leader. No need
       * to setpgid */
      if (term_mode == NATIVE_PROC_TERM_MODE_NONE) {
        /*
         * Put this process in its own process group.  This is done so that
         * ctrl+c from the terminal will not be delivered to the process, insuring
         * that it will not die when using gdb on a collapsed model.
         */
        r = setpgid(0, 0);
        if (r == -1) {
          int err = errno;
          rwmain_trace_crit(
              rwmain,
              "Failed to setpgid to own process group: %s",
              strerror(err));
        }
        RW_ASSERT(r != -1);
      }
    }

    rwmain_trace_info(rwmain, "execv %s", path);

    if (stream_paths) {
      FILE * ret;

      if (stream_paths->stdout_path) {
        ret = freopen(stream_paths->stdout_path, "w", stdout);
        if (!ret) {
          int err = errno;

          rwmain_trace_crit(
              rwmain,
              "Failed to redirect stdout to %s: %s",
              stream_paths->stdout_path,
              strerror(err));
          RW_CRASH();
        }
      }

      if (stream_paths->stderr_path) {
        if (!strcasecmp(stream_paths->stderr_path, "stdout"))
          ret = freopen(stream_paths->stdout_path, "a", stderr);
        else
          ret = freopen(stream_paths->stderr_path, "w", stderr);

        if (!ret) {
          int err = errno;

          rwmain_trace_crit(
              rwmain,
              "Failed to redirect stderr to %s: %s",
              stream_paths->stdout_path,
              strerror(err));
          RW_CRASH();
        }
      }
    }

    if (!envp)
      execvp(path, argv);
    else
      execvpe(path, argv, env);

    fprintf(stderr, "execv failed: cwd = %s, exe = %s, error = %s", get_current_dir_name(), path, strerror(errno));
    rwmain_trace_error(
        rwmain,
        "execv failed: cwd = %s, exe = %s, error = %s",
        get_current_dir_name(),
        path, strerror(errno));

    close(pipe_fds[1]);
    exit(1);

  } else if (track) {
    rw_status_t status;
    struct rwmain_proc * np = NULL;

    close(pipe_fds[1]);

    np = rwmain_proc_alloc(rwmain, instance_name, is_rwproc, lpid, pipe_fds[0]);
    RW_ASSERT(np);

    status = RW_SKLIST_INSERT(&(rwmain->procs), np);
    RW_ASSERT(status == RW_STATUS_SUCCESS);
  }

  if (lpid != 0 && reap) {
    r = reaper_client_add_pid(rwvcs->reaper_sock, lpid);
    if (r) {
      rwmain_trace_crit(
          rwmain,
          "Failed to send pid %d to reaper: %s",
          lpid,
          strerror(r));
      RW_CRASH();
    }
  }

  if (pid)
    *pid = lpid;

  if (env)
    free(env);

  return RW_STATUS_SUCCESS;
}

static char ** add_valgrind(
    const char ** argv,
    const char ** valgrind_args,
    size_t n_valgrind_args,
    const char ** prefix)
{
  char ** ret;
  size_t len = 0;
  size_t index;

  len = n_valgrind_args + 2;
  for (size_t i = 0;  argv[i]; ++len, i++) {;}
  for (size_t i = 0; prefix && prefix[i]; ++len, ++i) {;}

  ret = (char **)malloc(len * sizeof(char *));
  if (!ret) {
    RW_CRASH();
    return NULL;
  }
  bzero(ret, len * sizeof(char *));

  for (index = 0; prefix && prefix[index]; ++index) {
    ret[index] = strdup(prefix[index]);
    if (!ret[index]) {
      RW_CRASH();
      goto err;
    }
  }

  ret[index++] = strdup("valgrind");
  if (!ret[index - 1]) {
    RW_CRASH();
    goto err;
  }

  for (size_t i = 0; i < n_valgrind_args; ++i, ++index) {
    ret[index] = strdup(valgrind_args[i]);
    if (!ret[index]) {
      RW_CRASH();
      goto err;
    }
  }

  for (size_t i = 0; argv[i]; ++i, ++index) {
    ret[index] = strdup(argv[i]);
    if (!ret[index]) {
      RW_CRASH();
      goto err;
    }
  }

  return ret;

err:
  for (size_t i = 0; ret[i]; ++i)
    free(ret[i]);
  free(ret);

  return NULL;
}


rw_status_t open_slave_pty(struct rwmain_gi * rwmain, int master_pty_fd)
{
  const int term_name_size = 256;
  char term_name[term_name_size];
  int ret = -1;
  int slave_fd = -1;

  /* Get the slave terminal name and open it */
  ret = ptsname_r(master_pty_fd, term_name, term_name_size);
  if (ret != 0) {
    rwmain_trace_error(rwmain, "Failed to obtain Pseudo Terminal Name: %s", 
                       strerror(errno));
    return RW_STATUS_FAILURE;
  }

  slave_fd = open(term_name, O_RDWR);
  if (slave_fd == -1) {
    rwmain_trace_error(rwmain, "Failed to open PTY slave: %s", strerror(errno));
    return RW_STATUS_FAILURE;
  }

  // Master FD is no longer required in the child process
  close(master_pty_fd);

  // PTY becomes standard input (0)
  ret = dup2(slave_fd, STDIN_FILENO);
  if (ret == -1) {
    rwmain_trace_error(rwmain, "Failed to set PTY as STDIN: %s", 
                      strerror(errno));
    return RW_STATUS_FAILURE;
  }

  // PTY becomes standard output (1)
  ret = dup2(slave_fd, STDOUT_FILENO);
  if (ret == -1) {
    rwmain_trace_error(rwmain, "Failed to set PTY as STDOUT: %s", 
                      strerror(errno));
    return RW_STATUS_FAILURE;
  }

  // PTY becomes standard error (2)
  ret = dup2(slave_fd, STDERR_FILENO); 
  if (ret == -1) {
    rwmain_trace_error(rwmain, "Failed to set PTY as STDERR: %s", 
                      strerror(errno));
    return RW_STATUS_FAILURE;
  }

  close(slave_fd); // Don't need the slave fd anymore

  // Make the current process a new session leader
  ret = setsid();
  if (ret == -1) {
    rwmain_trace_error(rwmain, "Failed to set as session leader: %s", 
                      strerror(errno));
    return RW_STATUS_FAILURE;
  }

  // As the child is a session leader, set the controlling terminal to be 
  // the slave side of the PTY (now it is the stdin)
  // (Mandatory for programs like the shell to make them manage correctly their outputs)
  ret = ioctl(STDIN_FILENO, TIOCSCTTY, 1);
  if (ret == -1) {
    rwmain_trace_error(rwmain, "Failed to set controlling terminal: %s", 
                      strerror(errno));
    return RW_STATUS_FAILURE;
  }
  
  return RW_STATUS_SUCCESS;
}

rw_status_t start_terminal_io_tasklet(
                struct rwmain_gi *rwmain, 
                const char* parent_id)
{
  rw_status_t ret, status;
  vcs_manifest_tasklet m_tasklet;
  vcs_manifest_action  m_action;
  vcs_manifest_action_start m_start;

  vcs_manifest_tasklet__init(&m_tasklet);
  vcs_manifest_action__init(&m_action);
  vcs_manifest_action_start__init(&m_start);

  // ATTN: This information has to come from somewhere else, should not be
  // hardcoding it here
  m_tasklet.plugin_directory = "./usr/lib/rift/plugins/rwtermio-c";
  m_tasklet.plugin_name      = "rwtermio-c";

  m_action.start = &m_start;
  m_start.component_name = "RW.TermIO";
  m_start.has_config_ready = true;
  m_start.config_ready = true;
  m_start.has_recovery_action = true;
  m_start.recovery_action = RWVCS_TYPES_RECOVERY_TYPE_FAILCRITICAL;
  m_start.has_data_storetype = true;
  m_start.data_storetype = RWVCS_TYPES_DATA_STORE_NOSTORE;

  char *instance_name = NULL;
  uint32_t instance_id = 0;
  rwmain_check_and_set_instance_name(rwmain, parent_id, parent_id, &m_action, &instance_name, &instance_id);
  if (!instance_name) {
    status = generate_instance_name_id(rwmain->rwvx->rwvcs, 
                                       m_action.start, 
                                       &instance_name, 
                                       &instance_id);
    RW_ASSERT(status == RW_STATUS_SUCCESS);
    if (status != RW_STATUS_SUCCESS) {
      rwmain_trace_crit(
          rwmain,
          "Failed to get instance name and id for %s, parent %s",
          m_action.start->component_name,
          parent_id);
      RW_CRASH();
    }
  }

  ret = rwtasklet_start(rwmain, 
                        parent_id, 
                        &m_tasklet, 
                        &m_action, 
                        instance_name, 
                        instance_id);
  if (ret != RW_STATUS_SUCCESS) {
    rwmain_trace_error(rwmain, "Failed to launch TermIO tasklet");
  }

  return ret;
}


static rw_status_t start_multivm_rwmain(
    struct rwmain_gi *rwmain,
    const char *collection_comp_name,
    const char *vm_name,
    const char *vm_ip_addr)
{
  rw_status_t status;
  char *instance_name = NULL;
  uint32_t instance_id;
  rwvcs_instance_ptr_t rwvcs;

  RW_ASSERT(rwmain);
  rwvcs = rwmain->rwvx->rwvcs;
  RW_ASSERT(rwvcs);
  RW_ASSERT(rwmain->parent_id);

  vcs_manifest_component *ret_collection;
  status = rwvcs_manifest_component_lookup(
      rwmain->rwvx->rwvcs,
      collection_comp_name,
      &ret_collection);
  RW_ASSERT(status == RW_STATUS_SUCCESS);
  RW_ASSERT (ret_collection->component_type == RWVCS_TYPES_COMPONENT_TYPE_RWCOLLECTION);

  vcs_manifest_component *ret_vm;
  status = rwvcs_manifest_component_lookup(
      rwvcs,
      vm_name,
      &ret_vm);
  RW_ASSERT(status == RW_STATUS_SUCCESS);
  RW_ASSERT (ret_vm->component_type == RWVCS_TYPES_COMPONENT_TYPE_RWVM);

  bool found_collection = false;
  char *collection_name;
  rw_component_info self;
  rw_component_info component;
  status = rwvcs_rwzk_lookup_component(rwvcs, rwmain->parent_id, &self);
  RW_ASSERT(status == RW_STATUS_SUCCESS);
  for (size_t num_components=0; num_components<self.n_rwcomponent_children; ++num_components) {
    status = rwvcs_rwzk_lookup_component(rwvcs, self.rwcomponent_children[num_components], &component);
    //printf("%s <-> %s\n", collection_comp_name, component.component_name);
    if (component.component_type == RWVCS_TYPES_COMPONENT_TYPE_RWCOLLECTION) {
      if (!strcmp(component.component_name, collection_comp_name)) {
        found_collection = true;
        collection_name = self.rwcomponent_children[num_components];
        break;
      }
    }
  }
  if (!found_collection) {
    // Cookup a manifest-component info for COLLECTION to start
    // with event-name='onentry' and 'action.start'
    vcs_manifest_component *m_component;
    m_component = (vcs_manifest_component*)protobuf_c_message_duplicate(
        NULL,
        &ret_collection->base,
        ret_collection->base.descriptor);
    RW_ASSERT(m_component != NULL);

    char *python_variable_p[1] = {0};
    int r = asprintf(&python_variable_p[0], "vm_ip_address = '%s'", vm_ip_addr);
    RW_ASSERT(r != -1);
    vcs_manifest_action_start start; vcs_manifest_action_start__init(&start);
    start.component_name = (char *)vm_name;
    start.n_python_variable++;
    start.python_variable = python_variable_p;

    vcs_manifest_action action; vcs_manifest_action__init(&action);
    vcs_manifest_action *action_p[] = {&action};
    action.name = "Start MULTI-VM Collection";
    action.start = &start;

    vcs_manifest_event event; vcs_manifest_event__init(&event);
    event.n_action ++;
    event.name = "onentry";
    event.action = action_p;

    vcs_manifest_event_list event_list; vcs_manifest_event_list__init(&event_list);
    event_list.n_event ++;
    vcs_manifest_event *event_p[] = {&event};
    event_list.event = event_p;

#if 0
    // If the ret_collection allready has event-name='onentry' and
    // 'action.start', then append one more 'action.start'
    if (m_component->rwcollection->event_list) {
      vcs_manifest_event_list *mm_event_list = m_component->rwcollection->event_list;
      vcs_manifest_event *mm_event;
      status = rwvcs_manifest_event_lookup("onentry", mm_event_list, &mm_event);
      if (status == RW_STATUS_SUCCESS) {
        vcs_manifest_action **mm_action = mm_event->action;
        mm_action = realloc(mm_action, sizeof(vcs_manifest_action **)*(mm_event->n_action+1));
        mm_action[mm_event->n_action] = &action;
        mm_event->n_action ++;
      } else {
        RW_ASSERT(status == RW_STATUS_SUCCESS);
      }
    } else
#endif
    {
      m_component->rwcollection->event_list = &event_list;
    }

    // Cookup an m_action
    vcs_manifest_action  m_action;
    vcs_manifest_action_start m_start;

    vcs_manifest_action__init(&m_action);
    vcs_manifest_action_start__init(&m_start);

    m_action.start = &m_start;
    m_start.component_name = (char *)collection_comp_name;

    /* Start the collection with the additional VM on vm_ip_addr */
    status = generate_instance_name_id(
        rwvcs,
        m_action.start,
        &instance_name,
        &instance_id);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    status = rwcollection_start(
        rwmain,
        rwmain->parent_id,
        m_component->rwcollection,
        &m_action,
        instance_name,
        instance_id);
    // FIXME - free the cookedup m_component - it leaks now
  } else {
    // Cookup an m_action
    vcs_manifest_action  m_action;
    vcs_manifest_action_start m_start;

    vcs_manifest_action__init(&m_action);
    vcs_manifest_action_start__init(&m_start);

    m_action.start = &m_start;
    m_start.component_name = (char *)vm_name;

    status = generate_instance_name_id(
        rwvcs,
        m_action.start,
        &instance_name,
        &instance_id);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    char *python_variables[2];
    int r;
    r = asprintf(&python_variables[0], "vm_ip_address = '%s'", vm_ip_addr);
    RW_ASSERT(r != -1);
    python_variables[1] = 0;
    status = rwvcs_variable_list_evaluate(
        rwvcs,
        1,
        python_variables);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    /* Start the additional VM on vm_ip_addr */
    status = rwvm_start(
        rwmain,
        collection_name,
        ret_vm->rwvm,
        &m_action,
        instance_name,
        instance_id);
  }
  return status;
}

typedef struct {
  struct rwmain_gi *rwmain;
  char *collection_name;
  char *vm_name;
  char *vm_ip_addr;
} start_multivm_rwmain_s;

static void start_multivm_rwmain_f(void *ctx)
{
  rw_status_t status;
  start_multivm_rwmain_s *ud = (start_multivm_rwmain_s*)ctx;
  status =  start_multivm_rwmain(
      ud->rwmain,
      ud->collection_name,
      ud->vm_name,
      ud->vm_ip_addr);
  RW_ASSERT(status == RW_STATUS_SUCCESS);
  free(ud->collection_name);
  free(ud->vm_name);
  free(ud->vm_ip_addr);
  free(ud);
}

static void watcher_multivm_rwmain(void* ud)
{
  RW_ASSERT(ud);
  struct rwmain_gi *rwmain = (struct rwmain_gi *)ud;
  rw_status_t status;
  int idx = 0;

  char *collection_name = NULL;
  char *vm_name = NULL;
  char *vm_ip_addr = NULL;

  char ** children = NULL;
  char *rwzk_get_data = NULL;

  status = rwvcs_rwzk_get(rwmain->rwvx->rwvcs, multivm_rwzk_path, &rwzk_get_data);

  status = rwvcs_rwzk_get_children(rwmain->rwvx->rwvcs, multivm_rwzk_path, &children);
  RW_ASSERT(status == RW_STATUS_SUCCESS || status == RW_STATUS_NOTFOUND);
  if (children) {
    while (children[idx] != NULL) {
      char node_rwzk_path[256] = {0};
      sprintf(node_rwzk_path, "%s/%s", multivm_rwzk_path, children[idx]);

      status = rwvcs_rwzk_get(rwmain->rwvx->rwvcs, node_rwzk_path, &rwzk_get_data);

      struct rwmain_multivm *rt = NULL;
      char key[sizeof(rt->key)];
      strncpy(key, rwzk_get_data, sizeof(key)-1);

      char *p = rwzk_get_data;
      collection_name = p;
      p = strchr(collection_name, ':');
      if (!p) goto fail;
      p[0] = '\0';
      p++;

      vm_name = p;
      p = strchr(vm_name, ':');
      if (!p) goto fail;
      p[0] = '\0';
      p++;

      vm_ip_addr = p;
      p = strchr(vm_ip_addr, ':');
      if (p) p[0] = '\0';

      status = RW_SKLIST_LOOKUP_BY_KEY(&(rwmain->multivms), key, &rt);
      if (status != RW_STATUS_SUCCESS) {
        struct rwmain_multivm *multivm = RW_MALLOC0(sizeof(*multivm));
        strncpy(multivm->key, key, sizeof(multivm->key)-1);
        status = RW_SKLIST_INSERT(&(rwmain->multivms), multivm);
        RW_ASSERT(status == RW_STATUS_SUCCESS);

        start_multivm_rwmain_s *ud = malloc(sizeof(*ud));
        RW_ASSERT(ud);
        ud->rwmain = rwmain;
        ud->collection_name = strdup(collection_name);
        ud->vm_name = strdup(vm_name);
        ud->vm_ip_addr = strdup(vm_ip_addr);

        rwsched_dispatch_async_f(
            rwmain->rwvx->rwsched_tasklet,
            rwsched_dispatch_get_main_queue(rwmain->rwvx->rwsched),
            ud,
            start_multivm_rwmain_f);
      }
      idx++;
    }
  }
  goto done;

fail:
  status = RW_STATUS_FAILURE;
done:
  if (rwzk_get_data)
    free(rwzk_get_data);
  if (children) {
    idx = 0;
    while (children[idx]) {
      free(children[idx]);
      idx++;
    }
    free(children);
  }
  return;
}

static void
rwmain_inform_current_children(rwvcs_instance_ptr_t rwvcs,
                               rwdts_xact_block_t   *block,
                               rw_component_info    *cinfo,
                               vcs_vm_state         vm_state)
{
  rw_status_t status;
  if ((cinfo->component_type == RWVCS_TYPES_COMPONENT_TYPE_RWTASKLET) 
      || (cinfo->component_type == RWVCS_TYPES_COMPONENT_TYPE_PROC)) {
    status = rwha_api_add_modeinfo_query_to_block(
        block,
        rwvcs->instance_name,
        cinfo->instance_name,
        vm_state);
    RW_ASSERT(status == RW_STATUS_SUCCESS);
  }

  int indx;
  rw_component_info chinfo;
  for(indx=0; indx < cinfo->n_rwcomponent_children; indx++) {
    status = rwvcs_rwzk_lookup_component(rwvcs, cinfo->rwcomponent_children[indx], &chinfo);
    RW_ASSERT(status == RW_STATUS_SUCCESS);
    rwmain_inform_current_children (rwvcs, block, &chinfo, vm_state);
  }
}


static void 
rwmain_reparent_instance(rwvcs_instance_ptr_t rwvcs,
                         rw_component_info *chinfo,
                         char *old_parent_name,
                         char *new_parent_name)
{
  rw_status_t status = rwvcs_rwzk_delete_child(rwvcs, old_parent_name, chinfo->component_name, chinfo->instance_id);
  RW_ASSERT(status == RW_STATUS_SUCCESS);
  status = rwvcs_rwzk_add_child(rwvcs, new_parent_name, chinfo->instance_name);
  RW_ASSERT(status == RW_STATUS_SUCCESS);
  RW_FREE(chinfo->rwcomponent_parent);
  chinfo->rwcomponent_parent = strdup(new_parent_name);
}

static int
rwmain_setup_restart_list(struct rwmain_gi *rwmain,
                          char *instance_name,
                          rwmain_restart_instance_t **restart_list,
                          char *failed_vm,
                          int restart_list_len,
                          bool skip_components)
{
  rwvcs_instance_ptr_t rwvcs = rwmain->rwvx->rwvcs;
  rw_component_info cinfo;
  rw_status_t status = rwvcs_rwzk_lookup_component(rwvcs, instance_name, &cinfo);
  RW_ASSERT(status == RW_STATUS_SUCCESS);

  if (!skip_components || !RWMAIN_SKIP_COMPONENTS(cinfo, failed_vm)) {
    rwmain_restart_instance_t *restart_instance =
      RW_MALLOC0_TYPE(sizeof(rwmain_restart_instance_t), rwmain_restart_instance_t);
    RW_ASSERT_TYPE(restart_instance, rwmain_restart_instance_t);

    restart_instance->instance_name = strdup(cinfo.instance_name);
    restart_instance->rwmain = rwmain;

    if (!(*restart_list)) {
      (*restart_list) = restart_instance;
      restart_list_len += 1;
    } else {
      restart_instance->next_restart_instance = (*restart_list);
      (*restart_list) = restart_instance;
      restart_list_len += 1;
    }
  }

  for(int indx=0; indx < cinfo.n_rwcomponent_children; indx++) {
    restart_list_len = rwmain_setup_restart_list(rwmain, 
                                                 cinfo.rwcomponent_children[indx], 
                                                 restart_list, 
                                                 failed_vm, 
                                                 restart_list_len, 
                                                 skip_components);
  }

  protobuf_free_stack(cinfo);
  return restart_list_len;
}

static void
rwmain_setup_recovery_children(struct rwmain_gi *rwmain,
                               char *instance_name,
                               char *failed_vm,
                               char *current_standby_vm)
{
  rwvcs_instance_ptr_t rwvcs = rwmain->rwvx->rwvcs;
  bool changed = false;
  rw_component_info cinfo;
  rw_status_t status = rwvcs_rwzk_lookup_component(rwvcs, instance_name, &cinfo);
  RW_ASSERT(status == RW_STATUS_SUCCESS);
  if (!RWMAIN_SKIP_COMPONENTS(cinfo, failed_vm)) {
    changed = true;
    cinfo.has_recovery_action = true;
    cinfo.recovery_action = RWVCS_TYPES_RECOVERY_TYPE_RESTART;
    if (!strcmp(cinfo.rwcomponent_parent, failed_vm)) {
      rwmain_reparent_instance(rwvcs, &cinfo, failed_vm, current_standby_vm);
    }
  }
  else if (strcmp(cinfo.instance_name, failed_vm)) {
    if (cinfo.proc_info) {
      RW_FREE(cinfo.proc_info);
      cinfo.proc_info = NULL;
    }
    status = rwvcs_rwzk_node_update(rwvcs, &cinfo);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    rw_component_info dummy;
    kill_component(rwmain, cinfo.instance_name, &dummy);
    protobuf_free_stack(dummy);
    if (cinfo.rwcomponent_parent) {
      rwvcs_rwzk_delete_child(rwvcs, cinfo.rwcomponent_parent, cinfo.component_name, cinfo.instance_id);
    }
    rwvcs_component_delete(rwvcs, &cinfo);
  }
  if (changed) {
    status = rwvcs_rwzk_node_update(rwvcs, &cinfo);
  }
  int indx;
  for(indx=0; indx < cinfo.n_rwcomponent_children; indx++) {
    rwmain_setup_recovery_children (rwmain, cinfo.rwcomponent_children[indx], failed_vm, current_standby_vm);
  }
  protobuf_free_stack(cinfo);
}

void
rwmain_inform_state_change_local (rwvcs_instance_ptr_t rwvcs,
                                  rw_component_info *cinfo,
                                  vcs_vm_state vm_state) 
{
  rwdts_xact_t *xact = rwdts_api_xact_create(
      rwvcs->apih,
      RWDTS_XACT_FLAG_TRACE | RWDTS_XACT_FLAG_ADVISE,
      NULL,
      NULL);
  rwdts_xact_block_t *block = rwdts_xact_block_create(xact);
  RW_ASSERT(block);
  rwmain_inform_current_children(
      rwvcs,
      block,
      cinfo,
      vm_state);
  rw_status_t status = rwdts_xact_block_execute(block, RWDTS_XACT_FLAG_END, NULL, 0, NULL); 
  RW_ASSERT(status == RW_STATUS_SUCCESS);
}

static void
rwmain_vmstate_change_notify(rwdts_api_t *apih, char *instance_name, vcs_vm_state state)
{
  RWPB_M_MSG_DECL_PTR_ALLOC(RwVcs_notif_VmstateChange, notif);
  notif->name = strdup(instance_name);
  notif->vmstate = state;
  notif->has_vmstate = TRUE;
  rwdts_api_send_notification(apih, &(notif->base));
  protobuf_c_message_free_unpacked(NULL, &(notif->base));
}

static restart_list_answer create_restart_list(struct rwmain_gi *rwmain, 
                                               rwvcs_instance_ptr_t rwvcs,
                                               rw_component_info* pinfo,
                                               bool skip_components)
{
  restart_list_answer answer;
  // Build the restart list i.e list of tasklets which needs to be started on this
  // and delete those entries from Zookeeper as well
  //
  int max_len = 0;
  int max_len_idx = 0; // The node for which length of restart_list was maximum
  rwmain_restart_instance_t *tmp_restart_list;

  //TODO/FIXME:
  //Initially the below 'for-loop' was written assuming LS mode only. So, it
  //worked okay when standby got promoted to master, there was no ambiguity
  //regarding the formation of restart_list, it just had to get all the components
  //of the 'other' node (former leader) from the zookeper.
  //But, for the LSS it is bit tricky. RiftWare tasklets works on the knowledge
  //of 'component-name' whereas pacemaker works with the knowledge of 'ip-address'.
  //So, when we come to the below loop, the current node (promoted to master) has no
  //knowledge of which of the other 2 nodes was 'Master', because all it knows is the
  //list of component names and does not store the corresponding IP-address mapped against
  //the component-name.
  //
  //So, as a WA, we are now getting the component list from both the nodes and then are
  //comparing the lengths of the obtained restart list. The length of the restart list obtained
  //from the node which was previously the master would be greater than the other one.

  int indx;
  for(indx=0; indx < pinfo->n_rwcomponent_children; indx++) {
    if (strcmp(pinfo->rwcomponent_children[indx], rwvcs->instance_name) != 0) { // IF the previous leader
      rwvcs->mgmt_info.restart_list = NULL;
      int restart_list_len = rwmain_setup_restart_list(rwmain,
                                       pinfo->rwcomponent_children[indx],
                                       &rwvcs->mgmt_info.restart_list,
                                       pinfo->rwcomponent_children[indx],
                                       0,
                                       skip_components);

      if (restart_list_len > max_len) {
        max_len = restart_list_len;
        max_len_idx = indx;
        tmp_restart_list = rwvcs->mgmt_info.restart_list;
      }
    }
  }

  answer.restart_list = tmp_restart_list;
  answer.index = max_len_idx;

  return answer;
}

static rw_status_t 
handle_common_transition_tasks(struct rwmain_gi *rwmain,
                               rwvcs_instance_ptr_t rwvcs,
                               vcs_vm_state state,
                               rw_component_info* pinfo)
{
  rw_status_t status = RW_STATUS_SUCCESS;
  restart_list_answer rlist_ans;

  // CRITICAL REGION
  // --------------------
  // 1. Create and acquire a lock in Zookeeper.
  // 2. If lock already created, wait for some time.
  // 3. Once lock acquired, create the restart list.
  // 4. Release the lock.
  // 5. Get the DTS router from the restart list and deregister it
  {
    int r; 

    struct timeval tv = { .tv_sec = 120, .tv_usec = 1000 };

    if (rwvcs_rwzk_exists(rwvcs, RW_ZK_TRANSITION_LOCK_PATH)) {
      status = rwvcs_rwzk_lock_path(rwvcs, RW_ZK_TRANSITION_LOCK_PATH, &tv);
      if (status != RW_STATUS_SUCCESS) {
        fprintf (stderr, "Failed to get lock on transition lock path\n");
        return RW_STATUS_FAILURE;
      }

    } else {
      // Take the lock
      status = rwvcs_rwzk_create(rwvcs, RW_ZK_TRANSITION_LOCK_PATH);
      if (status != RW_STATUS_SUCCESS) {
        fprintf (stderr, "Failed to create transition lock\n");
        return RW_STATUS_FAILURE;
      }
      status = rwvcs_rwzk_lock_path(rwvcs, RW_ZK_TRANSITION_LOCK_PATH, &tv);
      if (status != RW_STATUS_SUCCESS) {
        fprintf (stderr, "Failed to get lock on transition lock path\n");
        return RW_STATUS_FAILURE;
      }

    }

    // Create the restart list
    // Only for standby VMs
    if (state != RWVCS_TYPES_VM_STATE_MGMTACTIVE) {
      rlist_ans = create_restart_list(rwmain, rwvcs, pinfo, false);
      rwmain_restart_instance_t *restart_list = rlist_ans.restart_list;
      RW_ASSERT (restart_list);

      while (restart_list) {
        rw_component_info cinfo;
        status = rwvcs_rwzk_lookup_component(rwmain->rwvx->rwvcs, restart_list->instance_name, &cinfo);
        RW_ASSERT (status == RW_STATUS_SUCCESS);

        // FIXME: This code is repeated, both here and in heartbeat.c
        if (strcmp(cinfo.component_name, "dtsrouter") == 0) {
          int vm_id = split_instance_id(cinfo.rwcomponent_parent);
          char* dts_router_path = NULL;
          r = asprintf(&dts_router_path, "/R/RW.DTSRouter/%d", vm_id);
          RW_ASSERT(r != -1);

          status = rwdts_member_deregister_path(
                rwmain->dts,
                dts_router_path,
                cinfo.has_recovery_action ? cinfo.recovery_action : RWVCS_TYPES_RECOVERY_TYPE_FAILCRITICAL);

          RW_ASSERT(status == RW_STATUS_SUCCESS);

          free(dts_router_path);
        }
        restart_list = restart_list->next_restart_instance;
      }
    }

    // Remove the Lock
    rwvcs_rwzk_unlock_path(rwvcs, RW_ZK_TRANSITION_LOCK_PATH);
  }

  if (rwmain->redis_handle) {
    rwmain_redis_notify_transition(rwmain, state);
  }

  return RW_STATUS_SUCCESS;
}

static rw_status_t
wait_for_all_others_to_complete(struct rwmain_gi *rwmain,
                                rwvcs_instance_ptr_t rwvcs,
                                rw_component_info* pinfo)
{

  static int timer = 0;

  //FIXME / TODO
  // Need some kind of barrier mechanism to make sure all the currently
  // running standby VMs are done with the common transition tasks.
  // Currently just waiting for some time..

  // Wait for SOME TIME
  if (timer < 10) {
    timer++;
    return RW_STATUS_FAILURE;
  }
  timer = 0;

  (void) pinfo;

  rw_status_t status = rwvcs_rwzk_delete_path(rwvcs, RW_ZK_TRANSITION_LOCK_PATH);
  // The above delete can only be done by one VM, so we can assert
  // if that is not the case.
  RW_ASSERT (status == RW_STATUS_SUCCESS);

  return RW_STATUS_SUCCESS;
}

static void rwmain_notify_transition_async_f(void* ud)
{
  RW_ASSERT (ud);
  transition_state_info* state_info = (transition_state_info*) ud;
  struct rwmain_gi* rwmain = state_info->rwmain;
  vcs_vm_state state = state_info->vm_state;

  rwvcs_instance_ptr_t rwvcs = rwmain->rwvx->rwvcs;
  rw_status_t status = RW_STATUS_SUCCESS;

  rw_component_info cinfo;
  status = rwvcs_rwzk_lookup_component(rwvcs, rwvcs->instance_name, &cinfo);

  if (status != RW_STATUS_SUCCESS) {
    fprintf (stderr, "Failed to get data from zookeeper..trying again\n");
    goto restart_task;
    return;
  }

  switch (state_info->fsm_state) {
  case STARTING:
  {
    //Update the FSM state
    state_info->fsm_state = HANDLE_COMMON_TASKS;
  }
  /******* FALLTHROUGH *********/
  case HANDLE_COMMON_TASKS:
  {
    rw_component_info tmp_pinfo;
    status = rwvcs_rwzk_lookup_component(rwvcs, cinfo.rwcomponent_parent, &tmp_pinfo);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    // Perform common transition tasks, which includes:
    // 1. Notifying redis about state change.
    // 2. Iterating over the restart list to find the
    // Master router and to delete all its registrations.
    status = handle_common_transition_tasks(rwmain, rwvcs, state, &tmp_pinfo);

    if (status != RW_STATUS_SUCCESS) {
      protobuf_free_stack(cinfo);
      fprintf(stderr, "Retrying transition tasks\n");
      goto restart_task;
      return;
    }
    // update the FSM state
    state_info->fsm_state = WAIT_FOR_OTHERS;
  }
  /******* FALLTHROUGH *********/
  case WAIT_FOR_OTHERS:
  {
    rw_component_info tmp_pinfo;
    status = rwvcs_rwzk_lookup_component(rwvcs, cinfo.rwcomponent_parent, &tmp_pinfo);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    // Wait for all other standby VM's to finish, else will result
    // in incorrect behaviour if the Master VM goes on to delete the
    // Zookeeper entries from here on.
    // Kind of a barrier impl.
    if (state == RWVCS_TYPES_VM_STATE_MGMTACTIVE) {
      status = wait_for_all_others_to_complete(rwmain, rwvcs, &tmp_pinfo);
      if (status != RW_STATUS_SUCCESS) {
        protobuf_free_stack(cinfo);
        fprintf (stderr, "Waiting for all VM to finish their transition tasks...\n");
        goto restart_task;
        return;
      }
    }
    break;
  }

  default:
    RW_ASSERT (0);

  };

  rw_component_info pinfo;

  if (state == RWVCS_TYPES_VM_STATE_MGMTACTIVE) {

    status = rwvcs_rwzk_lookup_component(rwvcs, cinfo.rwcomponent_parent, &pinfo);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    // Notify and publish the state change
    rwmain_vmstate_change_notify(rwmain->dts, rwvcs->instance_name, state);
    rwvcs_publish_active_mgmt_info(rwvcs);

    ck_pr_store_int(&rwvcs->restart_inprogress, 1);
    // Notify the children under this component about
    // the HA state change.
    rwmain_inform_state_change_local(rwvcs, &cinfo, state);

    cinfo.vm_info->has_leader = true;
    cinfo.vm_info->leader = true;
    cinfo.vm_info->has_vm_state = true;
    cinfo.vm_info->vm_state = RWVCS_TYPES_VM_STATE_MGMTACTIVE;

    // Update this component info in zookeeper
    status = rwvcs_rwzk_node_update(rwvcs, &cinfo);
    RW_ASSERT(status == RW_STATUS_SUCCESS);

    restart_list_answer rlist_ans = create_restart_list(rwmain, rwvcs, &pinfo, true);

    rwvcs->mgmt_info.restart_list = rlist_ans.restart_list;
    RW_ASSERT(rwvcs->mgmt_info.restart_list);

    rwmain_setup_recovery_children(rwmain,
        pinfo.rwcomponent_children[rlist_ans.index],
        pinfo.rwcomponent_children[rlist_ans.index],
        rwvcs->instance_name);

    // Kill/stop the tasklet and delete its entry from
    // Zookeeper
    rw_component_info dummy;
    kill_component(rwmain, pinfo.rwcomponent_children[rlist_ans.index], &dummy);

    if (dummy.rwcomponent_parent) {
      rwvcs_rwzk_delete_child(rwvcs, dummy.rwcomponent_parent, dummy.component_name, dummy.instance_id);
    }

    rwvcs_component_delete(rwvcs, &dummy);
    protobuf_free_stack(dummy);

    // Iterate through the restart list and stop those tasklets
    rwmain_restart_instance_t *restart_instance = rwvcs->mgmt_info.restart_list;
    while (restart_instance) {
      RW_ASSERT_TYPE(restart_instance, rwmain_restart_instance_t);
      rw_component_info dummy;
      process_component_death(rwmain, restart_instance->instance_name, &dummy);
      protobuf_free_stack(dummy);
      restart_instance = restart_instance->next_restart_instance;
    }

    // Do special handling for Mgmt Agent i.e
    // remove uAgent from the restart list since it is
    // already running in standby mode
    rwmain_restart_instance_t *uagent = NULL;
    rwmain_restart_instance_t *prev = NULL;
    restart_instance = rwvcs->mgmt_info.restart_list;

    while (restart_instance &&
           !strstr(restart_instance->instance_name, "uAgent"))
    {
      prev = restart_instance;
      restart_instance = restart_instance->next_restart_instance;
    }

    if (restart_instance) {
      uagent = restart_instance;
      if (prev) {
        prev->next_restart_instance = restart_instance->next_restart_instance;
      } else {
        rwvcs->mgmt_info.restart_list = restart_instance->next_restart_instance;
      }
      uagent->next_restart_instance = NULL;
      RW_FREE(uagent->instance_name);
      RW_FREE_TYPE(uagent, rwmain_restart_instance_t);
    }
  }

  protobuf_free_stack(cinfo);
  free(state_info);

  return;
  RW_ASSERT (0);

restart_task:
   rwsched_dispatch_after_f(
        rwmain->rwvx->rwsched_tasklet,
        dispatch_time(DISPATCH_TIME_NOW, NSEC_PER_SEC * 2),
        rwsched_dispatch_get_main_queue(rwmain->rwvx->rwsched),
        state_info,
        rwmain_notify_transition_async_f
       );
  return;
}

void rwmain_notify_transition(struct rwmain_gi* rwmain, vcs_vm_state state)
{
  transition_state_info* state_info = (transition_state_info*) malloc(sizeof(transition_state_info));
  RW_ASSERT (state_info);

  state_info->rwmain = rwmain;
  state_info->vm_state = state;
  state_info->fsm_state = STARTING;

  rwsched_dispatch_async_f(
      rwmain->rwvx->rwsched_tasklet,
      rwsched_dispatch_get_main_queue(rwmain->rwvx->rwsched),
      state_info,
      rwmain_notify_transition_async_f);

  return;
}
